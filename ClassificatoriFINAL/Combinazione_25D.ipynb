{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_curve, roc_auc_score, precision_recall_curve, auc, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, SelectPercentile, f_classif, f_regression, SelectFromModel\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import ttest_ind\n",
    "from xgboost import XGBClassifier\n",
    "import statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CSV/data_rad_clin_DEF.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../CSV/data_rad_clin_DEF.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels_column\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CSV/data_rad_clin_DEF.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"../CSV/data_rad_clin_DEF.csv\"\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "labels_column = data['label']\n",
    "labels = labels_column.astype(int).tolist()\n",
    "labels=np.array(labels)\n",
    "\n",
    "# Estrazione dei numeri dai nomi dei pazienti\n",
    "loaded_patients = data['IDs_new'].str.extract(r'(\\d+)').astype(int).squeeze().tolist()\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Patient Names: \", loaded_patients )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Percorso del file .npy\n",
    "file_path = \"../indici_aree/aree_05.npy\"\n",
    "arraytuple = np.load(file_path)\n",
    "\n",
    "# Converti il primo elemento di ogni tupla in int\n",
    "arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "# Percorso del file zip\n",
    "zip_file_path = \"../CSV/EncodersAllSlices/RESNET50_ALL_SLICES.zip\"\n",
    "\n",
    "# Apri il file zip e leggi il CSV direttamente dalla memoria\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Ottieni il nome del file CSV dentro lo zip\n",
    "    csv_filename = zip_ref.namelist()[0]\n",
    "    # Leggi il file CSV direttamente dalla memoria\n",
    "    with zip_ref.open(csv_filename) as csv_file:\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "# Droppa l'ultima colonna\n",
    "data = data.drop(data.columns[-1], axis=1)\n",
    "\n",
    "# Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di arraytuple\n",
    "filtered_data = data[\n",
    "    data.apply(lambda row: any((row['Patient'] == (t[0]) and row['Slice'] == t[1]) for t in arraytuple), axis=1)\n",
    "]\n",
    "\n",
    "# Filter columns that start with 'original' or 'dentro'\n",
    "filtered_patients = []\n",
    "\n",
    "for patient_id in loaded_patients:\n",
    "    # Filter the data for the specific patient\n",
    "    filtered_patient_data = filtered_data[filtered_data['Patient'] == patient_id]\n",
    "    \n",
    "    # Dopo aver filtrato per paziente, rimuovi le prime due colonne ('Patient' e 'Slice')\n",
    "    filtered_patient_data = filtered_patient_data.drop(filtered_patient_data.columns[:2], axis=1)\n",
    "    \n",
    "    slices = []\n",
    "    \n",
    "    for _, slice_row in filtered_patient_data.iterrows():\n",
    "        # Select only the filtered columns for each slice\n",
    "        slice_features = slice_row.tolist()\n",
    "        slices.append(slice_features)\n",
    "    \n",
    "    filtered_patients.append(slices)\n",
    "\n",
    "# Stampa per verificare che le prime due colonne siano state rimosse\n",
    "print(filtered_patient_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funzione per creare array da array di array\n",
    "def continue_array(filtered_patients, labels):\n",
    "    all_features = []\n",
    "    for patient in filtered_patients:\n",
    "        for image_features in patient:\n",
    "            all_features.append(image_features)\n",
    "\n",
    "    all_features_array = np.array(all_features)\n",
    "    expanded_labels = []\n",
    "    expanded_patient_ids = []\n",
    "\n",
    "    for i in range(len(filtered_patients)):\n",
    "        num_images = len(filtered_patients[i])\n",
    "        expanded_labels.extend([labels[i]] * num_images)\n",
    "        expanded_patient_ids.extend([loaded_patients[i]] * num_images)\n",
    "\n",
    "    expanded_labels_array = np.array(expanded_labels)\n",
    "    expanded_patient_ids_array = np.array(expanded_patient_ids)\n",
    "\n",
    "    return all_features_array, expanded_labels_array, expanded_patient_ids_array\n",
    "\n",
    "\n",
    "## funzioni per feature correlation\n",
    "def filter_highly_correlated_features(df, corr, threshold=0.85):\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    removed_features = []\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= threshold:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    removed_features.append(df.columns[j])\n",
    "\n",
    "    return removed_features\n",
    "\n",
    "\n",
    "def perform_correlation(z_train, y_train, numero = 32, threshold = 0.85):\n",
    "    all_images, _, _= continue_array(z_train, y_train)\n",
    "\n",
    "    df = pd.DataFrame(all_images, columns=[f'feature_{i}' for i in range(numero)])\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    features_selected = filter_highly_correlated_features(df, corr_matrix, threshold)\n",
    "    \n",
    "    return features_selected\n",
    "\n",
    "## funzione per rimuovere le features con p_value maggiore della treshold\n",
    "def select_features_by_p_value(x_train_expanded, y_train_expanded, p_value_threshold=0.05):\n",
    "\n",
    "    p_values = []\n",
    "    num_features = x_train_expanded.shape[1]\n",
    "\n",
    "    for i in range(num_features):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    selected_features_indices = np.where(p_values < p_value_threshold)[0]\n",
    "\n",
    "    sorted_indices = selected_features_indices[np.argsort(p_values[selected_features_indices])]\n",
    "\n",
    "    x_train_expanded = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_expanded, sorted_indices\n",
    "\n",
    "\n",
    "\n",
    "## funzione per rimozione di features specifiche\n",
    "def remove_features_from_patients(patients, features_to_remove):\n",
    "    feature_indices_to_remove = [int(feature.split('_')[1]) for feature in features_to_remove]\n",
    "    \n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.delete(image_features, feature_indices_to_remove, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))    \n",
    "\n",
    "    return final_patients\n",
    "\n",
    "\n",
    "## FEATURE SELECTION LASSO\n",
    "def select_features_with_lasso(X, y, alpha=0.001):\n",
    "    \n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    coefficients = lasso.coef_\n",
    "    selected_features = np.where(coefficients != 0)[0]\n",
    "    X_selected = X[:, selected_features]\n",
    "\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION LOGISTIC\n",
    "def logistic_regression_feature_selection(X, y, num_features):\n",
    "    lr = LogisticRegression(max_iter=2000, random_state=42)\n",
    "    lr.fit(X, y)\n",
    "    coef_abs = np.abs(lr.coef_)\n",
    "    feature_importances = np.mean(coef_abs, axis=0)\n",
    "    selected_features = feature_importances.argsort()[-num_features:][::-1]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION MRMR\n",
    "def mrmr_feature_selection(X, y, num_features):\n",
    "    mi = mutual_info_classif(X, y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    distances = squareform(pdist(X_scaled.T, 'euclidean'))\n",
    "    \n",
    "    selected_features = []\n",
    "    selected_indices = []\n",
    "\n",
    "    first_feature_index = np.argmax(mi)\n",
    "    selected_features.append(first_feature_index)\n",
    "    selected_indices.append(first_feature_index)\n",
    "    \n",
    "    for _ in range(num_features - 1):\n",
    "        max_relevance = -np.inf\n",
    "        selected_feature_index = -1\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            relevance = mi[i]\n",
    "            redundancy = np.mean(distances[i, selected_indices])\n",
    "            \n",
    "            mrmr_score = relevance - redundancy\n",
    "            \n",
    "            if mrmr_score > max_relevance:\n",
    "                max_relevance = mrmr_score\n",
    "                selected_feature_index = i\n",
    "        \n",
    "        selected_features.append(selected_feature_index)\n",
    "        selected_indices.append(selected_feature_index)\n",
    "\n",
    "    X_selected = X[:, selected_indices]\n",
    "    return X_selected, selected_indices\n",
    "\n",
    "## FEATURE SELECTION RANDOM FOREST\n",
    "def rf_feature_selection(X, y, num_features):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    feature_importances = rf.feature_importances_\n",
    "    selected_features = np.argsort(feature_importances)[-num_features:][::-1]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "\n",
    "## FEATURE SELECTION P_VALUE\n",
    "# Seleziona e ordina le feature basate sui p-value con un test t di Student poi \n",
    "# ordina le feature in base al p-value in ordine crescente e seleziona le prime `num_features` caratteristiche.\n",
    "\n",
    "def p_value_feature_selection(x_train_expanded, y_train_expanded, num_features):\n",
    "    p_values = []\n",
    "    num_features_total = x_train_expanded.shape[1]\n",
    "\n",
    "    # Calcolo dei p-value per ciascuna feature\n",
    "    for i in range(num_features_total):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    # Ordinare tutte le caratteristiche in base ai p-value (dal più piccolo al più grande)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_indices = sorted_indices[:num_features]\n",
    "\n",
    "    x_train_selected = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_selected, sorted_indices\n",
    "\n",
    "\n",
    "\n",
    "## funzione per lasciare solo le features indicate per array di array\n",
    "def keep_features_in_patients(patients, features_to_keep):\n",
    "\n",
    "    feature_indices_to_keep = [int(feature) for feature in features_to_keep]\n",
    "\n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.take(image_features, feature_indices_to_keep, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))\n",
    "\n",
    "    return final_patients\n",
    "\n",
    "\n",
    "## funzione per lasciare solo le features indicate per array\n",
    "def filter_patients_features(filtered_patients, selected_features):\n",
    "    filtered_patients_selected = []\n",
    "    \n",
    "    for patient_features in filtered_patients:\n",
    "        # Select only the features specified in selected_features\n",
    "        patient_features_selected = patient_features[:, selected_features]\n",
    "        filtered_patients_selected.append(patient_features_selected)\n",
    "\n",
    "    return filtered_patients_selected\n",
    "\n",
    "\n",
    "def classifierinitialization(classifier):\n",
    "    if classifier == 'RandomForest':\n",
    "                            classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif classifier == 'Logistic':\n",
    "                            classi = LogisticRegression(random_state=42, max_iter=2000)\n",
    "    elif classifier == 'SVM':\n",
    "                            classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    elif classifier == 'XgBoost':\n",
    "                            classi = XGBClassifier(random_state=42)\n",
    "    elif classifier == 'MLP':\n",
    "                            classi = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive', activation = 'logistic')\n",
    "    elif classifier == 'ensemble':\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                            logistic_model = LogisticRegression(random_state=42, max_iter=2000)\n",
    "                            svc_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                            classi = VotingClassifier(\n",
    "                                estimators=[\n",
    "                                    ('random_forest', rf_model),\n",
    "                                    ('logistic', logistic_model),\n",
    "                                    ('svc', svc_model)\n",
    "                                ],\n",
    "                                voting='soft'\n",
    "                                )\n",
    "    return classi\n",
    "\n",
    "## funzione per effettuarr majority voting o mean su tutte le slice di un paziente, per passare da una predizione sulla slice\n",
    "## alla predizione per il paziente\n",
    "\n",
    "def prob_to_binary(predictions_proba, patient_scores, threshold, mode):\n",
    "    final_predictionarray = []          \n",
    "    \n",
    "    if mode == 'MV':  # Majority Voting\n",
    "        for p in predictions_proba:\n",
    "            test_patient_predictions = []\n",
    "            for proba in p:               \n",
    "                predictions_binary = 1 if proba[0][1] > threshold else 0\n",
    "                test_patient_predictions.append(predictions_binary)\n",
    "            count_0 = np.sum(np.array(test_patient_predictions) == 0) \n",
    "            count_1 = np.sum(np.array(test_patient_predictions) == 1)                                   \n",
    "            final_prediction = 0 if count_0 > count_1 else 1\n",
    "            final_predictionarray.append(final_prediction)\n",
    "    \n",
    "    elif mode == 'Mean':  # Mean of probabilities\n",
    "        for score in patient_scores:\n",
    "            predictions_binary = 1 if score > threshold else 0\n",
    "            final_predictionarray.append(predictions_binary)\n",
    "    \n",
    "    elif mode == 'Max':  # Maximum probability across both classes\n",
    "        for p in predictions_proba:\n",
    "            max_proba = None\n",
    "            max_slice = None\n",
    "\n",
    "            # Itera su ogni slice per trovare la massima probabilità (indipendentemente dalla classe)\n",
    "            for proba in p:\n",
    "                class_0_prob = proba[0][0]  # Probabilità della classe 0\n",
    "                class_1_prob = proba[0][1]  # Probabilità della classe 1\n",
    "                \n",
    "                # Trova la probabilità massima tra entrambe le classi per ciascuna slice\n",
    "                slice_max_proba = max(class_0_prob, class_1_prob)\n",
    "\n",
    "                # Trova la slice con la probabilità massima\n",
    "                if max_proba is None or slice_max_proba > max_proba:\n",
    "                    max_proba = slice_max_proba\n",
    "                    max_slice = proba  # Memorizza la slice con la probabilità massima\n",
    "\n",
    "            # Ora usa la probabilità della classe 1 della slice con la probabilità massima per il confronto con la soglia\n",
    "            predictions_binary = 1 if max_slice[0][1] > threshold else 0\n",
    "            final_predictionarray.append(predictions_binary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return final_predictionarray\n",
    "\n",
    "\n",
    "\n",
    "def classification_method(selector, alpha, classifier, x_train_expanded, y_train_expanded, patients_test, y_test, features_test, num_features, modePrediction, thresholds=np.arange(0.001, 0.501, 0.001), mode = \"Val\", selected_features= [0]):\n",
    "\n",
    "    #print(features_test)\n",
    "    if(mode == \"Val\"):\n",
    "            selected_features = None \n",
    "            \n",
    "            if num_features != len(x_train_expanded[0]) or alpha != 0:\n",
    "                if selector == \"lasso\":\n",
    "                    X_selected, selected_features = select_features_with_lasso(x_train_expanded, y_train_expanded, alpha)\n",
    "                    if(len(selected_features)==0):\n",
    "                            return 0\n",
    "                elif selector == \"logistic\": \n",
    "                    X_selected, selected_features = logistic_regression_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector == \"mrmr\":\n",
    "                    X_selected, selected_features = mrmr_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector == \"rf\":\n",
    "                    X_selected, selected_features = rf_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector==\"p_value\":\n",
    "                    X_selected, selected_features= p_value_feature_selection(x_train_expanded,y_train_expanded, num_features)\n",
    "                else:\n",
    "                    print(\"Wrong selector. Choose between: mrmr, rf, logistic, p_value, lasso\")\n",
    "                    return\n",
    "                \n",
    "                #features_test = filter_patients_features(features_test, selected_features)\n",
    "                features_test = keep_features_in_patients(features_test, selected_features)\n",
    "                keep_features_in_patients\n",
    "                #print(f\"features_test {features_test}\")\n",
    "            else:\n",
    "                X_selected = x_train_expanded\n",
    "                selected_features = list(range(len(x_train_expanded[0])))  # Selezioniamo tutte le feature se non si fa feature selection\n",
    "            number_features = len(selected_features) \n",
    "        \n",
    "            smote = SMOTE(random_state=42)\n",
    "\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_selected, y_train_expanded)\n",
    "            classifier.fit(X_resampled, y_resampled)\n",
    "\n",
    "    patient_scores=[]\n",
    "    temp_array=[]\n",
    "\n",
    "    for x in range(len(patients_test)):\n",
    "        patient_predictions = []\n",
    "        patient_predictions1 = []\n",
    "        r=len(features_test[x])\n",
    "        for i in range(r):\n",
    "            dato = features_test[x][i].reshape(1, -1)\n",
    "            prediction = classifier.predict_proba(dato)\n",
    "            prediction1 = classifier.predict_proba(dato)[:,1]\n",
    "            patient_predictions.append(prediction)\n",
    "            patient_predictions1.append(prediction1)\n",
    "            mean=np.mean(patient_predictions1)\n",
    "\n",
    "        patient_scores.append(mean) ## contiene la media delle probabilità\n",
    "        temp_array.append(patient_predictions) ## contiene tutte le probabilità\n",
    "    \n",
    "    \n",
    "\n",
    "    best_f1_score = 0\n",
    "    best_threshold = None\n",
    "    #best_precision=0\n",
    "    #best_recall=0\n",
    "    best_prediction=[]\n",
    "    \n",
    "    # Valuta le performance utilizzando diverse threshold\n",
    "    if isinstance(thresholds, (int, float, complex)):\n",
    "        thresholds=[thresholds]\n",
    "\n",
    "    ## se non viene specificato usi il parametro di default \n",
    "    ## viene calcolata la threshold che porta a un f1 migliore e effettuata la prediction con quella\n",
    "    if(len(thresholds)!=1):\n",
    "        for threshold in thresholds:\n",
    "            binary_predictions = prob_to_binary(temp_array, patient_scores, threshold, modePrediction)\n",
    "            f1 = f1_score(y_test, binary_predictions)\n",
    "            if f1 > best_f1_score:\n",
    "                best_f1_score = f1\n",
    "                best_threshold = threshold\n",
    "                #best_precision = precision_score(y_test, binary_predictions)\n",
    "                #best_recall = recall_score(y_test, binary_predictions)\n",
    "                best_prediction=binary_predictions\n",
    "\n",
    "    else: ## qui vuol dire che è il set di test che usa la threshold migliore che viene passata dal validation\n",
    "        best_threshold = thresholds\n",
    "        best_prediction=prob_to_binary(temp_array, patient_scores, best_threshold, modePrediction)\n",
    "        best_f1_score = f1_score(y_test, best_prediction)\n",
    "        #best_precision = precision_score(y_test, best_prediction)\n",
    "        #best_recall = recall_score(y_test, best_prediction)\n",
    "\n",
    "    #print(f\"La migliore threshold è {best_threshold} con f1score di {best_f1_score} e precision {best_precision} e recall {best_recall}.\")\n",
    "    y_test= np.array(y_test)\n",
    "    best_prediction=np.array(best_prediction)\n",
    "    test_accuracy = accuracy_score(y_test, best_prediction)\n",
    "\n",
    "    #precision, recall, _ = precision_recall_curve(y_test, patient_scores) ## utili per ricostruire grafici\n",
    "    pr_auc = average_precision_score(y_test, patient_scores)\n",
    "\n",
    "    #fpr, tpr, _ = roc_curve(y_test, best_prediction)\n",
    "    roc_auc= roc_auc_score(y_test, patient_scores)\n",
    "\n",
    "    # Calcola la balanced accuracy\n",
    "    bal_acc = balanced_accuracy_score(y_test, best_prediction)\n",
    "\n",
    "    conf= confusion_matrix(y_test, best_prediction)\n",
    "\n",
    "\n",
    "    best_case = {\n",
    "                    'alpha': alpha,\n",
    "                    'num_features': number_features,\n",
    "                    'selected_features': selected_features,\n",
    "                    'pr_auc': pr_auc,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'f1': best_f1_score,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'confusion_matrix': conf,\n",
    "                    'best_threshold': best_threshold,\n",
    "                    'balanced accuracy': bal_acc\n",
    "                }\n",
    "    \n",
    "    return best_case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# patients_train contiene il nome dei pazienti (5,12 etc)\n",
    "# y_train contiene le labels\n",
    "# features_train contiene array di array dove ogni paziente ha varie immagini rappresentate da n features\n",
    "\n",
    "#patients_train1, patients_test, y_train1, y_test, features_train1, features_test= train_test_split(loaded_patients, labels, patients, test_size=0.3, shuffle=False)\n",
    "\n",
    "patients_train1, patients_test, y_train1, y_test, features_train1, features_test= train_test_split(loaded_patients, labels, filtered_patients, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(\"Number of train patients: \", len(features_train1))\n",
    "print(\"Number of test patients: \", len(features_test))\n",
    "\n",
    "\n",
    "print(\"Number of features for every image: \", len(features_train1[0][0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### correlation 0.8\n",
    "starting_features = len(features_train1[0][0])\n",
    "features=perform_correlation(features_train1, y_train1, starting_features, 0.9)\n",
    "\n",
    "final_patients_train1=remove_features_from_patients(features_train1, features)\n",
    "final_patients_test=remove_features_from_patients(features_test, features)\n",
    "\n",
    "print(f\"correlation ha trovato {final_patients_train1[0].shape[1]} features\\n\")\n",
    "\n",
    "### p-value 0.01\n",
    "x_train_expanded1, y_train_expanded1, _ = continue_array(final_patients_train1, y_train1)\n",
    "x_train_expanded1, sf= select_features_by_p_value(x_train_expanded1, y_train_expanded1, 0.05)\n",
    "print(f\"Scelte {len(sf)} features dal p_value\\n\")\n",
    "print(\"x_train_expanded1\", x_train_expanded1.shape)\n",
    "\n",
    "final_patients_test=keep_features_in_patients(final_patients_test, sf)\n",
    "final_patients_train1=keep_features_in_patients(final_patients_train1, sf)\n",
    "\n",
    "print(\"final_patients_test\", final_patients_test[0].shape)\n",
    "print(\"final_patients_train1\", final_patients_train1[0].shape)\n",
    "\n",
    "patients_train1 = np.array(patients_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import functools\n",
    "\n",
    "def select_and_save_features(input_npy, input_zip, loaded_patients, results_val_others, results_val_lasso, original_features, output_csv_filename):\n",
    "    # Carica il file .npy contenente le tuple (Paziente, Slice, altro)\n",
    "    arraytuple = np.load(input_npy)\n",
    "    # Converti il primo elemento di ogni tupla in intero\n",
    "    arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "    # Apri il file zip e leggi il CSV\n",
    "    with zipfile.ZipFile(input_zip, 'r') as zip_ref:\n",
    "        # Ottieni il nome del file CSV dentro lo zip\n",
    "        csv_filename = zip_ref.namelist()[0]\n",
    "        # Leggi il file CSV direttamente dalla memoria\n",
    "        with zip_ref.open(csv_filename) as csv_file:\n",
    "            data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Droppa l'ultima colonna (se necessario)\n",
    "    data = data.drop(data.columns[-1], axis=1)\n",
    "\n",
    "    # Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di arraytuple\n",
    "    filtered_data = data[\n",
    "        data.apply(lambda row: any((row['Patient'] == t[0] and row['Slice'] == t[1]) for t in arraytuple), axis=1)\n",
    "    ]\n",
    "\n",
    "    # Inizia la grid search con i selettori non-lasso\n",
    "    num_features_range = list(range(1, 30))\n",
    "    grid_results_others = {}\n",
    "    grid_results_lasso = {}\n",
    "\n",
    "    selectors_others = ['mrmr', 'rf', 'logistic']\n",
    "    classifiers = set([res['classifier'] for res in results_val_others] + [res['classifier'] for res in results_val_lasso])\n",
    "    alpha_values = list(set(res['alpha'] for res in results_val_lasso if 'alpha' in res))\n",
    "    modes = set([res['mode'] for res in results_val_others + results_val_lasso])  # Assumi che 'mode' sia un parametro comune\n",
    "\n",
    "    # Itera su results_val_others\n",
    "    for classifier in classifiers:\n",
    "        for selector in selectors_others:\n",
    "            for mode in modes:\n",
    "                for num_features in num_features_range:\n",
    "                    filtered_results = [\n",
    "                        res for res in results_val_others if (\n",
    "                            res['classifier'] == classifier and\n",
    "                            res['selector'] == selector and\n",
    "                            res['num_features'] == num_features and\n",
    "                            res['mode'] == mode\n",
    "                        )\n",
    "                    ]\n",
    "                    if filtered_results:\n",
    "                        f1_values = [res['f1'] for res in filtered_results]\n",
    "                        balaccuracy_values = [res['balanced accuracy'] for res in filtered_results]\n",
    "                        roc_values = [res['roc_auc'] for res in filtered_results]\n",
    "\n",
    "                        avg_f1 = sum(f1_values) / len(f1_values)\n",
    "                        avg_balaccuracy = sum(balaccuracy_values) / len(balaccuracy_values)\n",
    "                        avg_roc = sum(roc_values) / len(roc_values)\n",
    "\n",
    "                        std_f1 = statistics.stdev(f1_values) if len(f1_values) > 1 else 0\n",
    "                        std_balaccuracy = statistics.stdev(balaccuracy_values) if len(balaccuracy_values) > 1 else 0\n",
    "                        std_roc_auc = statistics.stdev(roc_values) if len(roc_values) > 1 else 0\n",
    "\n",
    "                        grid_results_others[(classifier, selector, num_features, mode)] = {\n",
    "                            'avg_f1': avg_f1,\n",
    "                            'std_f1': std_f1,\n",
    "                            'avg_balaccuracy': avg_balaccuracy,\n",
    "                            'std_balaccuracy': std_balaccuracy,\n",
    "                            'avg_roc_auc': avg_roc,\n",
    "                            'std_roc_auc': std_roc_auc\n",
    "                        }\n",
    "\n",
    "    # Itera su results_val_lasso\n",
    "    for classifier in classifiers:\n",
    "        for mode in modes:\n",
    "            for alpha in alpha_values:\n",
    "                filtered_results = [\n",
    "                    res for res in results_val_lasso if (\n",
    "                        res['classifier'] == classifier and\n",
    "                        res['alpha'] == alpha and\n",
    "                        res['mode'] == mode\n",
    "                    )\n",
    "                ]\n",
    "                if filtered_results:\n",
    "                    f1_values = [res['f1'] for res in filtered_results]\n",
    "                    balaccuracy_values = [res['balanced accuracy'] for res in filtered_results]\n",
    "                    roc_values = [res['roc_auc'] for res in filtered_results]\n",
    "\n",
    "                    avg_f1 = sum(f1_values) / len(f1_values)\n",
    "                    avg_balaccuracy = sum(balaccuracy_values) / len(balaccuracy_values)\n",
    "                    avg_roc = sum(roc_values) / len(roc_values)\n",
    "\n",
    "                    std_f1 = statistics.stdev(f1_values) if len(f1_values) > 1 else 0\n",
    "                    std_balaccuracy = statistics.stdev(balaccuracy_values) if len(balaccuracy_values) > 1 else 0\n",
    "                    std_roc_auc = statistics.stdev(roc_values) if len(roc_values) > 1 else 0\n",
    "\n",
    "                    grid_results_lasso[(classifier, 'lasso', alpha, mode)] = {\n",
    "                        'avg_f1': avg_f1,\n",
    "                        'std_f1': std_f1,\n",
    "                        'avg_balaccuracy': avg_balaccuracy,\n",
    "                        'std_balaccuracy': std_balaccuracy,\n",
    "                        'avg_roc_auc': avg_roc,\n",
    "                        'std_roc_auc': std_roc_auc\n",
    "                    }\n",
    "\n",
    "    # Ordina i risultati\n",
    "    sorted_results_others = sorted(grid_results_others.items(), key=lambda x: (x[1]['avg_balaccuracy'], x[1]['avg_roc_auc']), reverse=True)\n",
    "    sorted_results_lasso = sorted(grid_results_lasso.items(), key=lambda x: (x[1]['avg_balaccuracy'], x[1]['avg_roc_auc']), reverse=True)\n",
    "\n",
    "    sorted_results = sorted_results_others + sorted_results_lasso\n",
    "\n",
    "    def sort_key(x):\n",
    "        result1 = round(x[1]['avg_balaccuracy'], 2)\n",
    "        result2 = round(x[1]['avg_roc_auc'], 2)\n",
    "        return (result1, result2)\n",
    "\n",
    "    sorted_results = sorted(sorted_results, key=sort_key, reverse=True)\n",
    "\n",
    "    # Seleziona il miglior risultato\n",
    "    best_combination = sorted_results[0]\n",
    "    params, metrics = best_combination\n",
    "\n",
    "    # Trova le selected_features per il fold 0\n",
    "    selected_features = []\n",
    "    if params[1] == 'lasso':\n",
    "        for res in results_val_lasso:\n",
    "            if (res['classifier'] == params[0] and res['alpha'] == params[2] and res['mode'] == params[3]):\n",
    "                selected_features = res['selected_features']\n",
    "                break\n",
    "    else:\n",
    "        for res in results_val_others:\n",
    "            if (res['classifier'] == params[0] and res['selector'] == params[1] and res['num_features'] == params[2] and res['mode'] == params[3]):\n",
    "                selected_features = res['selected_features']\n",
    "                break\n",
    "\n",
    "    # Seleziona gli indici originali delle feature\n",
    "    selected_original_features = [original_features[i] for i in selected_features]\n",
    "\n",
    "    # Seleziona le colonne corrispondenti dal dataframe filtrato\n",
    "    df_features_selected = filtered_data.iloc[:, selected_original_features]\n",
    "\n",
    "    # Aggiungi la colonna paziente e slice\n",
    "    df_selected = pd.concat([filtered_data[['Patient', 'Slice']], df_features_selected], axis=1)\n",
    "\n",
    "    # Rinomina 'Unnamed: 0' in 'Paziente'\n",
    "    df_selected.rename(columns={'Patient': 'Paziente'}, inplace=True)\n",
    "\n",
    "    # Ordina per il numero del paziente\n",
    "    df_selected = df_selected.sort_values(by=['Paziente', 'Slice'])\n",
    "\n",
    "    # Salva il file CSV\n",
    "    df_selected.to_csv(output_csv_filename, index=False)\n",
    "\n",
    "    print(f\"Le feature selezionate sono state salvate nel file '{output_csv_filename}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
