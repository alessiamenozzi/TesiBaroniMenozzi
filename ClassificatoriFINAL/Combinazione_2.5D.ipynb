{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import (accuracy_score, f1_score, roc_curve, roc_auc_score, precision_score, \n",
    "                             recall_score, confusion_matrix, precision_recall_curve, auc, \n",
    "                             average_precision_score)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, SelectPercentile, f_classif, f_regression, SelectFromModel\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import ttest_ind\n",
    "from xgboost import XGBClassifier\n",
    "import statistics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento Dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
      " 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1]\n",
      "Number of labels: 129\n",
      "Patient Names:  [5, 12, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 29, 30, 31, 33, 35, 36, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 68, 69, 70, 71, 74, 75, 76, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 127, 128, 129, 133, 135, 136, 137, 138, 139, 141, 142, 144, 146, 147, 149, 150, 153, 155, 158, 159, 161, 163, 166, 168, 169, 170, 171, 175, 176, 178, 182, 183, 188, 189, 190, 193, 197, 199, 200, 205]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"../CSV/data_rad_clin_DEF.csv\"\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "labels_column = data['label']\n",
    "labels = labels_column.astype(int).tolist()\n",
    "labels=np.array(labels)\n",
    "\n",
    "# Estrazione dei numeri dai nomi dei pazienti\n",
    "loaded_patients = data['IDs_new'].str.extract(r'(\\d+)').astype(int).squeeze().tolist()\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Patient Names: \", loaded_patients )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature_0  Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  \\\n",
      "2812  362.10083        0.0  154.24982   0.000000  23.756374  175.92972   \n",
      "2813  372.70490        0.0  136.19838   0.000000   2.629818  189.45522   \n",
      "2814  346.53735        0.0  115.48985   0.000000  13.140981  257.83862   \n",
      "2815  375.38477        0.0  127.85433   2.354725  53.831130  216.97185   \n",
      "2816  354.15894        0.0  162.53334   0.000000  63.148438  205.31361   \n",
      "\n",
      "      Feature_6  Feature_7   Feature_8  Feature_9  ...  Feature_502  \\\n",
      "2812  25.596645        0.0   71.050385   76.99930  ...    151.57822   \n",
      "2813   0.000000        0.0   53.569332   76.83872  ...    168.13292   \n",
      "2814   0.000000        0.0  102.440674  159.44064  ...    160.81201   \n",
      "2815   0.000000        0.0  161.432600  161.51317  ...    154.25906   \n",
      "2816   0.000000        0.0  206.874880  122.91578  ...    159.16074   \n",
      "\n",
      "      Feature_503  Feature_504  Feature_505  Feature_506  Feature_507  \\\n",
      "2812    29.014444     0.000000    17.908308     623.6441    11.734689   \n",
      "2813     0.000000     0.000000    13.740473     615.4914    34.069107   \n",
      "2814     0.000000     0.000000    29.771175     635.6449    29.641903   \n",
      "2815     0.000000     5.100542    62.502200     734.2473    27.629803   \n",
      "2816    64.413930     0.000000    30.434847     678.5563    47.148720   \n",
      "\n",
      "      Feature_508  Feature_509  Feature_510  Feature_511  \n",
      "2812     0.000000     0.000000   113.388480      0.00000  \n",
      "2813    15.729602     0.000000   116.155266      0.00000  \n",
      "2814    50.958930     0.000000   102.787800      0.00000  \n",
      "2815    51.075930     0.000000   106.479680      0.00000  \n",
      "2816    46.499775     1.460253   102.588806     18.30946  \n",
      "\n",
      "[5 rows x 512 columns]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Percorso del file .npy\n",
    "# Percorso del file .npy\n",
    "file_path = \"../indici_aree/aree_07.npy\"\n",
    "arraytuple = np.load(file_path)\n",
    "\n",
    "# Converti il primo elemento di ogni tupla in int\n",
    "arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "### 1C \n",
    "zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetFullImage/VGG_ALL_SLICES_Imagenet_FullImage.zip\"\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetFullImage/RESNET_ALL_SLICES_Imagenet_FullImage.zip\"\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetFullImage/INCEPTION_ALL_SLICES_Imagenet_FullImage.zip\"\n",
    "\n",
    "### 2B\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersNuoviPesiRitagliata/INCEPTION_ALL_SLICES_NuoviPesi_Ritagliata.zip\"\n",
    "\n",
    "###  1B\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetRitagliata/VGG_ALL_SLICES_Imagenet_Ritagliata.zip\"\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetRitagliata/RESNET_ALL_SLICES_Imagenet_Ritagliata.zip\"\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersImagenetRitagliata/INCEPTION_ALL_SLICES_Imagenet_Ritagliata.zip\"\n",
    "\n",
    "### 2C\n",
    "#zip_file_path = \"../CSV/2_5/GMP/EncodersNuoviPesiFullImage/INCRES_ALL_SLICES_NuoviPesi_FullImage.zip\"\n",
    "\n",
    "# Apri il file zip e leggi il CSV direttamente dalla memoria\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Ottieni il nome del file CSV dentro lo zip\n",
    "    csv_filename = zip_ref.namelist()[0]\n",
    "    # Leggi il file CSV direttamente dalla memoria\n",
    "    with zip_ref.open(csv_filename) as csv_file:\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "# Ordina i pazienti in ordine crescente\n",
    "arraytuple_sorted = sorted(arraytuple, key=lambda t: t[0])\n",
    "\n",
    "# Crea un dizionario con le slice e le loro aree, raggruppate per paziente\n",
    "patients_slices = {}\n",
    "for t in arraytuple_sorted:\n",
    "    patient_id = t[0]\n",
    "    if patient_id not in patients_slices:\n",
    "        patients_slices[patient_id] = []\n",
    "    patients_slices[patient_id].append(t)\n",
    "\n",
    "# Per ogni paziente, tieni solo le 5 slice con area maggiore\n",
    "for patient_id in patients_slices:\n",
    "    patients_slices[patient_id] = sorted(patients_slices[patient_id], key=lambda x: x[2], reverse=True)[:5]\n",
    "\n",
    "# Ora creiamo una lista finale di tuple per i pazienti ordinati con le 5 slice maggiori\n",
    "final_tuples = []\n",
    "for patient_id in sorted(patients_slices):\n",
    "    final_tuples.extend(patients_slices[patient_id])\n",
    "\n",
    "# Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di final_tuples\n",
    "filtered_data = data[\n",
    "    data.apply(lambda row: any((row['Patient'] == (t[0]) and row['Slice'] == t[1]) for t in final_tuples), axis=1)\n",
    "]\n",
    "\n",
    "# Percorso del secondo file zip contenente il secondo CSV\n",
    "second_zip_file_path = \"../CSV/GMP/EncodersAllSlices/Radiomica_Wavelet_25D.csv.zip\"\n",
    "\n",
    "# Apri il secondo file zip e leggi il CSV direttamente dalla memoria\n",
    "with zipfile.ZipFile(second_zip_file_path, 'r') as zip_ref:\n",
    "    # Ottieni il nome del file CSV dentro lo zip\n",
    "    second_csv_filename = zip_ref.namelist()[0]\n",
    "    # Leggi il file CSV direttamente dalla memoria\n",
    "    with zip_ref.open(second_csv_filename) as second_csv_file:\n",
    "        second_csv = pd.read_csv(second_csv_file)\n",
    "\n",
    "# Controlla che gli indici delle slice di ogni paziente siano presenti nel secondo CSV\n",
    "for patient_id in patients_slices:\n",
    "    slices_in_first = [t[1] for t in patients_slices[patient_id]]\n",
    "    slices_in_second = second_csv[second_csv['Paziente'] == patient_id]['Slice'].tolist()\n",
    "    \n",
    "    # Se uno degli indici di slice non è presente nel secondo CSV, stampa un errore\n",
    "    if not set(slices_in_first).issubset(set(slices_in_second)):\n",
    "        print(f\"Errore: alcune slice del paziente {patient_id} non sono presenti nel secondo CSV.\")\n",
    "        exit()\n",
    "\n",
    "# Creazione delle ultime variabili se il controllo è ok\n",
    "filtered_patients = []\n",
    "\n",
    "for patient_id in sorted(patients_slices):\n",
    "    # Filtra i dati per il paziente specifico\n",
    "    filtered_patient_data = filtered_data[filtered_data['Patient'] == patient_id]\n",
    "    \n",
    "    # Dopo aver filtrato per paziente, rimuovi le prime due colonne ('Patient' e 'Slice')\n",
    "    filtered_patient_data = filtered_patient_data.drop(filtered_patient_data.columns[:2], axis=1)\n",
    "    \n",
    "    slices = []\n",
    "    \n",
    "    for _, slice_row in filtered_patient_data.iterrows():\n",
    "        # Seleziona solo le colonne filtrate per ogni slice\n",
    "        slice_features = slice_row.tolist()\n",
    "        slices.append(slice_features)\n",
    "    \n",
    "    filtered_patients.append(slices)\n",
    "\n",
    "# Stampa per verificare che le prime due colonne siano state rimosse\n",
    "print(filtered_patient_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radiomica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      original_shape2D_Elongation  original_shape2D_MajorAxisLength  \\\n",
      "5177                     0.670387                         30.390451   \n",
      "5178                     0.646481                         31.754581   \n",
      "5179                     0.606961                         33.766174   \n",
      "5180                     0.558283                         35.566134   \n",
      "5181                     0.558702                         36.113805   \n",
      "\n",
      "      original_shape2D_MaximumDiameter  original_shape2D_MeshSurface  \\\n",
      "5177                         31.064449                         400.0   \n",
      "5178                         32.249031                         432.0   \n",
      "5179                         33.615473                         460.0   \n",
      "5180                         34.985711                         485.0   \n",
      "5181                         35.846897                         474.5   \n",
      "\n",
      "      original_shape2D_MinorAxisLength  original_shape2D_Perimeter  \\\n",
      "5177                         20.373362                  117.396970   \n",
      "5178                         20.528739                  117.396970   \n",
      "5179                         20.494737                  118.811183   \n",
      "5180                         19.855975                  120.811183   \n",
      "5181                         20.176872                  145.195959   \n",
      "\n",
      "      original_shape2D_PerimeterSurfaceRatio  original_shape2D_PixelSurface  \\\n",
      "5177                                0.293492                          400.0   \n",
      "5178                                0.271752                          432.0   \n",
      "5179                                0.258285                          460.0   \n",
      "5180                                0.249095                          485.0   \n",
      "5181                                0.305998                          474.0   \n",
      "\n",
      "      original_shape2D_Sphericity  original_firstorder_10Percentile  ...  \\\n",
      "5177                     0.603918                              38.0  ...   \n",
      "5178                     0.627610                              44.1  ...   \n",
      "5179                     0.639921                              40.0  ...   \n",
      "5180                     0.646203                              45.0  ...   \n",
      "5181                     0.531825                              59.3  ...   \n",
      "\n",
      "      wavelet-LL_glszm_SmallAreaHighGrayLevelEmphasis  \\\n",
      "5177                                       773.215986   \n",
      "5178                                      1030.141213   \n",
      "5179                                      1702.495865   \n",
      "5180                                      2138.917830   \n",
      "5181                                      1625.327261   \n",
      "\n",
      "      wavelet-LL_glszm_SmallAreaLowGrayLevelEmphasis  \\\n",
      "5177                                        0.015942   \n",
      "5178                                        0.018449   \n",
      "5179                                        0.006265   \n",
      "5180                                        0.007723   \n",
      "5181                                        0.009852   \n",
      "\n",
      "      wavelet-LL_glszm_ZoneEntropy  wavelet-LL_glszm_ZonePercentage  \\\n",
      "5177                      6.261107                         0.737500   \n",
      "5178                      6.407895                         0.743056   \n",
      "5179                      6.601274                         0.728261   \n",
      "5180                      6.709604                         0.725773   \n",
      "5181                      6.581662                         0.820675   \n",
      "\n",
      "      wavelet-LL_glszm_ZoneVariance  wavelet-LL_ngtdm_Busyness  \\\n",
      "5177                       1.307211                   0.032496   \n",
      "5178                       1.976999                   0.028105   \n",
      "5179                       2.377189                   0.016616   \n",
      "5180                       1.274850                   0.014585   \n",
      "5181                       0.474105                   0.014563   \n",
      "\n",
      "      wavelet-LL_ngtdm_Coarseness  wavelet-LL_ngtdm_Complexity  \\\n",
      "5177                     0.039159                  5239.929088   \n",
      "5178                     0.036993                  7363.036498   \n",
      "5179                     0.037106                 12084.828482   \n",
      "5180                     0.035977                 13339.682266   \n",
      "5181                     0.039691                 13562.035598   \n",
      "\n",
      "      wavelet-LL_ngtdm_Contrast  wavelet-LL_ngtdm_Strength  \n",
      "5177                   0.329919                 124.418521  \n",
      "5178                   0.396363                 160.860725  \n",
      "5179                   0.447991                 277.140702  \n",
      "5180                   0.564833                 257.154744  \n",
      "5181                   0.473861                 266.695842  \n",
      "\n",
      "[5 rows x 474 columns]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Percorso del file .npy\n",
    "file_path = \"../indici_aree/aree_07.npy\"\n",
    "arraytuple = np.load(file_path)\n",
    "\n",
    "# Converti il primo elemento di ogni tupla in int\n",
    "arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "# Percorso del file zip contenente il CSV\n",
    "zip_file_path = \"../CSV/GMP/EncodersAllSlices/Radiomica_Wavelet_25D.csv.zip\"\n",
    "\n",
    "# Apri il file zip e leggi il CSV direttamente dalla memoria\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    csv_filename = zip_ref.namelist()[0]  # Se c'è un solo file, puoi usare il primo\n",
    "    with zip_ref.open(csv_filename) as csv_file:\n",
    "        data = pd.read_csv(csv_file)\n",
    "\n",
    "# Crea un dizionario con le slice e le loro aree, raggruppate per paziente\n",
    "patients_slices = {}\n",
    "for t in arraytuple:\n",
    "    patient_id = t[0]\n",
    "    if patient_id not in patients_slices:\n",
    "        patients_slices[patient_id] = []\n",
    "    patients_slices[patient_id].append(t)\n",
    "\n",
    "# Per ogni paziente, tieni solo le 5 slice con area maggiore\n",
    "for patient_id in patients_slices:\n",
    "    patients_slices[patient_id] = sorted(patients_slices[patient_id], key=lambda x: x[2], reverse=True)[:5]\n",
    "\n",
    "# Crea una lista di tuple finali per i pazienti ordinati con le 5 slice maggiori\n",
    "final_tuples = []\n",
    "for patient_id in sorted(patients_slices):\n",
    "    final_tuples.extend(patients_slices[patient_id])\n",
    "\n",
    "# Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di final_tuples\n",
    "filtered_data = data[\n",
    "    data.apply(lambda row: any((row['Paziente'] == (t[0]) and row['Slice'] == t[1]) for t in final_tuples), axis=1)\n",
    "]\n",
    "\n",
    "# Filtra le colonne che iniziano con 'original' o 'dentro'\n",
    "filtered_patients = []\n",
    "for patient_id in loaded_patients:\n",
    "    # Filtra i dati per il paziente specifico\n",
    "    filtered_patient_data = filtered_data[filtered_data['Paziente'] == patient_id]\n",
    "    \n",
    "    # Rimuovi le prime due colonne ('Paziente' e 'Slice')\n",
    "    filtered_patient_data = filtered_patient_data.drop(filtered_patient_data.columns[:2], axis=1)\n",
    "    \n",
    "    # Rimuovi le colonne che iniziano con \"diagnostics\"\n",
    "    filtered_patient_data = filtered_patient_data.filter(regex='^(?!diagnostics).*')\n",
    "\n",
    "    slices = []\n",
    "    \n",
    "    for _, slice_row in filtered_patient_data.iterrows():\n",
    "        # Seleziona solo le colonne filtrate per ogni slice\n",
    "        slice_features = slice_row.tolist()\n",
    "        slices.append(slice_features)\n",
    "    \n",
    "    filtered_patients.append(slices)\n",
    "\n",
    "# Stampa per verificare che le colonne \"diagnostics\" siano state rimosse\n",
    "print(filtered_patient_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funzione per creare array da array di array\n",
    "def continue_array(filtered_patients, labels):\n",
    "    all_features = []\n",
    "    for patient in filtered_patients:\n",
    "        for image_features in patient:\n",
    "            all_features.append(image_features)\n",
    "\n",
    "    all_features_array = np.array(all_features)\n",
    "    expanded_labels = []\n",
    "    expanded_patient_ids = []\n",
    "\n",
    "    for i in range(len(filtered_patients)):\n",
    "        num_images = len(filtered_patients[i])\n",
    "        expanded_labels.extend([labels[i]] * num_images)\n",
    "        expanded_patient_ids.extend([loaded_patients[i]] * num_images)\n",
    "\n",
    "    expanded_labels_array = np.array(expanded_labels)\n",
    "    expanded_patient_ids_array = np.array(expanded_patient_ids)\n",
    "\n",
    "    return all_features_array, expanded_labels_array, expanded_patient_ids_array\n",
    "\n",
    "\n",
    "## funzioni per feature correlation\n",
    "def filter_highly_correlated_features(df, corr, threshold=0.85):\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    removed_features = []\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= threshold:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    removed_features.append(df.columns[j])\n",
    "\n",
    "    return removed_features\n",
    "\n",
    "\n",
    "def perform_correlation(z_train, y_train, numero = 32, threshold = 0.85):\n",
    "    all_images, _, _= continue_array(z_train, y_train)\n",
    "\n",
    "    df = pd.DataFrame(all_images, columns=[f'feature_{i}' for i in range(numero)])\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    features_selected = filter_highly_correlated_features(df, corr_matrix, threshold)\n",
    "    \n",
    "    return features_selected\n",
    "\n",
    "## funzione per rimuovere le features con p_value maggiore della treshold\n",
    "def select_features_by_p_value(x_train_expanded, y_train_expanded, p_value_threshold=0.05):\n",
    "\n",
    "    p_values = []\n",
    "    num_features = x_train_expanded.shape[1]\n",
    "\n",
    "    for i in range(num_features):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    selected_features_indices = np.where(p_values < p_value_threshold)[0]\n",
    "\n",
    "    sorted_indices = selected_features_indices[np.argsort(p_values[selected_features_indices])]\n",
    "\n",
    "    x_train_expanded = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_expanded, sorted_indices\n",
    "\n",
    "## funzione per rimozione di features specifiche\n",
    "def remove_features_from_patients(patients, features_to_remove):\n",
    "    feature_indices_to_remove = [int(feature.split('_')[1]) for feature in features_to_remove]\n",
    "    \n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.delete(image_features, feature_indices_to_remove, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))    \n",
    "\n",
    "    return final_patients\n",
    "\n",
    "\n",
    "## FEATURE SELECTION LASSO\n",
    "def select_features_with_lasso(X, y, alpha=0.001):\n",
    "    \n",
    "    lasso = Lasso(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    coefficients = lasso.coef_\n",
    "    selected_features = np.where(coefficients != 0)[0]\n",
    "    X_selected = X[:, selected_features]\n",
    "\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION LOGISTIC\n",
    "def logistic_regression_feature_selection(X, y, num_features):\n",
    "    lr = LogisticRegression(max_iter=2000, random_state=42)\n",
    "    lr.fit(X, y)\n",
    "    coef_abs = np.abs(lr.coef_)\n",
    "    feature_importances = np.mean(coef_abs, axis=0)\n",
    "    selected_features = feature_importances.argsort()[-num_features:][::-1]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "## FEATURE SELECTION MRMR\n",
    "def mrmr_feature_selection(X, y, num_features):\n",
    "    mi = mutual_info_classif(X, y, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    distances = squareform(pdist(X_scaled.T, 'euclidean'))\n",
    "    \n",
    "    selected_features = []\n",
    "    selected_indices = []\n",
    "\n",
    "    first_feature_index = np.argmax(mi)\n",
    "    selected_features.append(first_feature_index)\n",
    "    selected_indices.append(first_feature_index)\n",
    "    \n",
    "    for _ in range(num_features - 1):\n",
    "        max_relevance = -np.inf\n",
    "        selected_feature_index = -1\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            \n",
    "            relevance = mi[i]\n",
    "            redundancy = np.mean(distances[i, selected_indices])\n",
    "            \n",
    "            mrmr_score = relevance - redundancy\n",
    "            \n",
    "            if mrmr_score > max_relevance:\n",
    "                max_relevance = mrmr_score\n",
    "                selected_feature_index = i\n",
    "        \n",
    "        selected_features.append(selected_feature_index)\n",
    "        selected_indices.append(selected_feature_index)\n",
    "\n",
    "    X_selected = X[:, selected_indices]\n",
    "    return X_selected, selected_indices\n",
    "\n",
    "## FEATURE SELECTION RANDOM FOREST\n",
    "def rf_feature_selection(X, y, num_features):\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    feature_importances = rf.feature_importances_\n",
    "    selected_features = np.argsort(feature_importances)[-num_features:][::-1]\n",
    "    X_selected = X[:, selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "\n",
    "## FEATURE SELECTION P_VALUE\n",
    "# Seleziona e ordina le feature basate sui p-value con un test t di Student poi \n",
    "# ordina le feature in base al p-value in ordine crescente e seleziona le prime `num_features` caratteristiche.\n",
    "\n",
    "def p_value_feature_selection(x_train_expanded, y_train_expanded, num_features):\n",
    "    p_values = []\n",
    "    num_features_total = x_train_expanded.shape[1]\n",
    "\n",
    "    # Calcolo dei p-value per ciascuna feature\n",
    "    for i in range(num_features_total):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    # Ordinare tutte le caratteristiche in base ai p-value (dal più piccolo al più grande)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_indices = sorted_indices[:num_features]\n",
    "\n",
    "    x_train_selected = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_selected, sorted_indices\n",
    "\n",
    "\n",
    "\n",
    "## funzione per lasciare solo le features indicate per array di array\n",
    "def keep_features_in_patients(patients, features_to_keep):\n",
    "\n",
    "    feature_indices_to_keep = [int(feature) for feature in features_to_keep]\n",
    "\n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.take(image_features, feature_indices_to_keep, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))\n",
    "\n",
    "    return final_patients\n",
    "\n",
    "\n",
    "## funzione per lasciare solo le features indicate per array\n",
    "def filter_patients_features(filtered_patients, selected_features):\n",
    "    filtered_patients_selected = []\n",
    "    \n",
    "    for patient_features in filtered_patients:\n",
    "        # Select only the features specified in selected_features\n",
    "        patient_features_selected = patient_features[:, selected_features]\n",
    "        filtered_patients_selected.append(patient_features_selected)\n",
    "\n",
    "    return filtered_patients_selected\n",
    "\n",
    "\n",
    "def classifierinitialization(classifier):\n",
    "    if classifier == 'RandomForest':\n",
    "                            classi = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    elif classifier == 'Logistic':\n",
    "                            classi = LogisticRegression(random_state=42, max_iter=2000)\n",
    "    elif classifier == 'SVM':\n",
    "                            classi = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "    elif classifier == 'XgBoost':\n",
    "                            classi = XGBClassifier(random_state=42)\n",
    "    elif classifier == 'MLP':\n",
    "                            classi = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=1000, random_state=42, early_stopping=True, learning_rate='adaptive', activation = 'logistic')\n",
    "    elif classifier == 'ensemble':\n",
    "                            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                            logistic_model = LogisticRegression(random_state=42, max_iter=2000)\n",
    "                            svc_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "                            classi = VotingClassifier(\n",
    "                                estimators=[\n",
    "                                    ('random_forest', rf_model),\n",
    "                                    ('logistic', logistic_model),\n",
    "                                    ('svc', svc_model)\n",
    "                                ],\n",
    "                                voting='soft'\n",
    "                                )\n",
    "    return classi\n",
    "\n",
    "## funzione per effettuarr majority voting o mean su tutte le slice di un paziente, per passare da una predizione sulla slice\n",
    "## alla predizione per il paziente\n",
    "\n",
    "def prob_to_binary(predictions_proba, patient_scores, threshold, mode):\n",
    "    final_predictionarray = []          \n",
    "    \n",
    "    if mode == 'MV':  # Majority Voting\n",
    "        for p in predictions_proba:\n",
    "            test_patient_predictions = []\n",
    "            for proba in p:               \n",
    "                predictions_binary = 1 if proba[0][1] > threshold else 0\n",
    "                test_patient_predictions.append(predictions_binary)\n",
    "            count_0 = np.sum(np.array(test_patient_predictions) == 0) \n",
    "            count_1 = np.sum(np.array(test_patient_predictions) == 1)                                   \n",
    "            final_prediction = 0 if count_0 > count_1 else 1\n",
    "            final_predictionarray.append(final_prediction)\n",
    "    \n",
    "    elif mode == 'Mean':  # Mean of probabilities\n",
    "        for score in patient_scores:\n",
    "            predictions_binary = 1 if score > threshold else 0\n",
    "            final_predictionarray.append(predictions_binary)\n",
    "    \n",
    "    elif mode == 'Max':  # Maximum probability across both classes\n",
    "        for p in predictions_proba:\n",
    "            max_proba = None\n",
    "            max_slice = None\n",
    "\n",
    "            # Itera su ogni slice per trovare la massima probabilità (indipendentemente dalla classe)\n",
    "            for proba in p:\n",
    "                class_0_prob = proba[0][0]  # Probabilità della classe 0\n",
    "                class_1_prob = proba[0][1]  # Probabilità della classe 1\n",
    "                \n",
    "                # Trova la probabilità massima tra entrambe le classi per ciascuna slice\n",
    "                slice_max_proba = max(class_0_prob, class_1_prob)\n",
    "\n",
    "                # Trova la slice con la probabilità massima\n",
    "                if max_proba is None or slice_max_proba > max_proba:\n",
    "                    max_proba = slice_max_proba\n",
    "                    max_slice = proba  # Memorizza la slice con la probabilità massima\n",
    "\n",
    "            # Ora usa la probabilità della classe 1 della slice con la probabilità massima per il confronto con la soglia\n",
    "            predictions_binary = 1 if max_slice[0][1] > threshold else 0\n",
    "            final_predictionarray.append(predictions_binary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return final_predictionarray\n",
    "\n",
    "\n",
    "\n",
    "def classification_method(selector, alpha, classifier, x_train_expanded, y_train_expanded, patients_test, y_test, features_test, num_features, modePrediction, thresholds=np.arange(0.001, 0.501, 0.001), mode = \"Val\", selected_features= [0]):\n",
    "\n",
    "    #print(features_test)\n",
    "    if(mode == \"Val\"):\n",
    "            selected_features = None \n",
    "            \n",
    "            if num_features != len(x_train_expanded[0]) or alpha != 0:\n",
    "                if selector == \"lasso\":\n",
    "                    X_selected, selected_features = select_features_with_lasso(x_train_expanded, y_train_expanded, alpha)\n",
    "                    if(len(selected_features)==0):\n",
    "                            return 0\n",
    "                elif selector == \"logistic\": \n",
    "                    X_selected, selected_features = logistic_regression_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector == \"mrmr\":\n",
    "                    X_selected, selected_features = mrmr_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector == \"rf\":\n",
    "                    X_selected, selected_features = rf_feature_selection(x_train_expanded, y_train_expanded, num_features)\n",
    "                elif selector==\"p_value\":\n",
    "                    X_selected, selected_features= p_value_feature_selection(x_train_expanded,y_train_expanded, num_features)\n",
    "                else:\n",
    "                    print(\"Wrong selector. Choose between: mrmr, rf, logistic, p_value, lasso\")\n",
    "                    return\n",
    "                \n",
    "                #features_test = filter_patients_features(features_test, selected_features)\n",
    "                features_test = keep_features_in_patients(features_test, selected_features)\n",
    "                #keep_features_in_patients\n",
    "                #print(f\"features_test {features_test}\")\n",
    "            else:\n",
    "                X_selected = x_train_expanded\n",
    "                selected_features = list(range(len(x_train_expanded[0])))  # Selezioniamo tutte le feature se non si fa feature selection\n",
    "            number_features = len(selected_features) \n",
    "        \n",
    "            #smote = SMOTE(random_state=10) ##### aiuto!! tenerlo???\n",
    "\n",
    "            #X_resampled, y_resampled = smote.fit_resample(X_selected, y_train_expanded)\n",
    "            #classifier.fit(X_resampled, y_resampled)\n",
    "            classifier.fit(X_selected, y_train_expanded)\n",
    "\n",
    "    if(mode=='features'):\n",
    "        X_selected = x_train_expanded[:, selected_features]\n",
    "        features_test = keep_features_in_patients(features_test, selected_features)\n",
    "        number_features = len(selected_features)\n",
    "        classifier.fit(X_selected, y_train_expanded)\n",
    "\n",
    "\n",
    "    patient_scores=[]\n",
    "    temp_array=[]\n",
    "\n",
    "    for x in range(len(patients_test)):\n",
    "        patient_predictions = []\n",
    "        patient_predictions1 = []\n",
    "        r=len(features_test[x])\n",
    "        for i in range(r):\n",
    "            dato = features_test[x][i].reshape(1, -1)\n",
    "            prediction = classifier.predict_proba(dato)\n",
    "            prediction1 = classifier.predict_proba(dato)[:,1]\n",
    "            patient_predictions.append(prediction)\n",
    "            patient_predictions1.append(prediction1)\n",
    "            mean=np.mean(patient_predictions1)\n",
    "\n",
    "        patient_scores.append(mean) ## contiene la media delle probabilità\n",
    "        temp_array.append(patient_predictions) ## contiene tutte le probabilità\n",
    "    \n",
    "    \n",
    "\n",
    "    best_f1_score = 0\n",
    "    best_threshold = None\n",
    "    best_prediction=[]\n",
    "    \n",
    "    # Valuta le performance utilizzando diverse threshold\n",
    "    if isinstance(thresholds, (int, float, complex)):\n",
    "        thresholds=[thresholds]\n",
    "\n",
    "    ## se non viene specificato usi il parametro di default \n",
    "    ## viene calcolata la threshold che porta a un f1 migliore e effettuata la prediction con quella\n",
    "    if(len(thresholds)!=1):\n",
    "        for threshold in thresholds:\n",
    "            binary_predictions = prob_to_binary(temp_array, patient_scores, threshold, modePrediction)\n",
    "            f1 = f1_score(y_test, binary_predictions)\n",
    "            if f1 > best_f1_score:\n",
    "                best_f1_score = f1\n",
    "                best_threshold = threshold\n",
    "                #best_precision = precision_score(y_test, binary_predictions)\n",
    "                #best_recall = recall_score(y_test, binary_predictions)\n",
    "                best_prediction=binary_predictions\n",
    "\n",
    "    else: ## qui vuol dire che è il set di test che usa la threshold migliore che viene passata dal validation\n",
    "        best_threshold = thresholds\n",
    "\n",
    "        best_prediction=prob_to_binary(temp_array, patient_scores, best_threshold, modePrediction)\n",
    "        best_f1_score = f1_score(y_test, best_prediction)\n",
    "        #best_precision = precision_score(y_test, best_prediction)\n",
    "        #best_recall = recall_score(y_test, best_prediction)\n",
    "\n",
    "    #print(f\"La migliore threshold è {best_threshold} con f1score di {best_f1_score} e precision {best_precision} e recall {best_recall}.\")\n",
    "    y_test= np.array(y_test)\n",
    "    best_prediction=np.array(best_prediction)\n",
    "    test_accuracy = accuracy_score(y_test, best_prediction)\n",
    "\n",
    "    #precision, recall, _ = precision_recall_curve(y_test, patient_scores) ## utili per ricostruire grafici\n",
    "    pr_auc = average_precision_score(y_test, patient_scores)\n",
    "\n",
    "    #fpr, tpr, _ = roc_curve(y_test, best_prediction)\n",
    "    roc_auc= roc_auc_score(y_test, patient_scores)\n",
    "\n",
    "    # Calcola la balanced accuracy\n",
    "    bal_acc = balanced_accuracy_score(y_test, best_prediction)\n",
    "\n",
    "    conf= confusion_matrix(y_test, best_prediction)\n",
    "\n",
    "\n",
    "    best_case = {\n",
    "                    'alpha': alpha,\n",
    "                    'num_features': number_features,\n",
    "                    'selected_features': selected_features,\n",
    "                    'pr_auc': pr_auc,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'f1': best_f1_score,\n",
    "                    'accuracy': test_accuracy,\n",
    "                    'confusion_matrix': conf,\n",
    "                    'best_threshold': best_threshold,\n",
    "                    'balanced accuracy': bal_acc\n",
    "                }\n",
    "    \n",
    "    return best_case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train patients:  90\n",
      "Number of test patients:  39\n",
      "Number of features for every image:  474\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# patients_train contiene il nome dei pazienti (5,12 etc)\n",
    "# y_train contiene le labels\n",
    "# features_train contiene array di array dove ogni paziente ha varie immagini rappresentate da n features\n",
    "\n",
    "#patients_train1, patients_test, y_train1, y_test, features_train1, features_test= train_test_split(loaded_patients, labels, patients, test_size=0.3, shuffle=False)\n",
    "\n",
    "patients_train1, patients_test, y_train1, y_test, features_train1, features_test= train_test_split(loaded_patients, labels, filtered_patients, test_size=0.3, shuffle=False)\n",
    "\n",
    "print(\"Number of train patients: \", len(features_train1))\n",
    "print(\"Number of test patients: \", len(features_test))\n",
    "\n",
    "\n",
    "print(\"Number of features for every image: \", len(features_train1[0][0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlation ha trovato 120 features\n",
      "\n",
      "Scelte 73 features dal p_value\n",
      "\n",
      "x_train_expanded1 (450, 73)\n",
      "final_patients_test (5, 73)\n",
      "final_patients_train1 (5, 73)\n"
     ]
    }
   ],
   "source": [
    "### correlation 0.8\n",
    "starting_features = len(features_train1[0][0])\n",
    "original_features = np.arange(len(features_train1[0][0]))\n",
    "\n",
    "features=perform_correlation(features_train1, y_train1, starting_features, 0.9)\n",
    "\n",
    "\n",
    "indices = [int(feature.split('_')[1]) for feature in features]\n",
    "original_features = np.delete(original_features, indices)\n",
    "\n",
    "final_patients_train1=remove_features_from_patients(features_train1, features)\n",
    "final_patients_test=remove_features_from_patients(features_test, features)\n",
    "\n",
    "print(f\"correlation ha trovato {final_patients_train1[0].shape[1]} features\\n\")\n",
    "\n",
    "### p-value 0.01\n",
    "x_train_expanded1, y_train_expanded1, _ = continue_array(final_patients_train1, y_train1)\n",
    "x_train_expanded1, sf= select_features_by_p_value(x_train_expanded1, y_train_expanded1, 0.05)\n",
    "\n",
    "\n",
    "original_features = original_features[sf]\n",
    "\n",
    "print(f\"Scelte {len(sf)} features dal p_value\\n\")\n",
    "print(\"x_train_expanded1\", x_train_expanded1.shape)\n",
    "\n",
    "final_patients_test=keep_features_in_patients(final_patients_test, sf)\n",
    "final_patients_train1=keep_features_in_patients(final_patients_train1, sf)\n",
    "\n",
    "print(\"final_patients_test\", final_patients_test[0].shape)\n",
    "print(\"final_patients_train1\", final_patients_train1[0].shape)\n",
    "\n",
    "patients_train1 = np.array(patients_train1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_inc = np.linspace(0.0007, 0.006, 30).tolist() ## RANGE PER RESNET e INCEPETION\n",
    "\n",
    "alpha_vgg = np.linspace(0.005, 0.5, 30).tolist() ## RANGE PER VGG\n",
    "\n",
    "alpha_rad = np.linspace(0.05, 3, 30).tolist() ## range per radiomica\n",
    "\n",
    "alpha_res_inc_1C = np.linspace(0.08, 0.5, 30).tolist() \n",
    "alpha_vgg_1C = np.linspace(7, 40, 30).tolist() \n",
    "\n",
    "alpha_inc_2C = np.linspace(0.0009, 0.03, 30).tolist() \n",
    "alpha_res_fullimage_new_norm= np.linspace(0.006, 0.05, 30).tolist() ### QUELLO DEL 2D PER OTTENERE LE COMBINAZIONE 2C RESNET\n",
    "alpha_res_2C = np.linspace(0.01, 0.06, 30).tolist() \n",
    "alpha_rad=np.linspace(0.04, 1, 30).tolist()+np.linspace(1, 30, 30).tolist()\n",
    "\n",
    "alpha_res_fullimage_new_norm= np.linspace(0.006, 0.05, 30).tolist()\n",
    "\n",
    "alpha_res_2B_2_5= np.linspace(0.002, 0.03, 30).tolist()\n",
    "alpha_incres_2B_2_5= np.linspace(0.001, 0.006, 20).tolist() + np.linspace(0.006, 0.02, 10).tolist()\n",
    "\n",
    "####1B\n",
    "alpha_vgg_1B_2_5 = np.linspace(0, 0.005, 10)\n",
    "alpha_inc_1B_2_5 = np.linspace(0.04, 0.4, 30)\n",
    "alpha_res_1B_2_5 = np.linspace(0.2, 0.6, 30)\n",
    "\n",
    "### 2C\n",
    "alpha_incres_2C_2_5= np.linspace(0.002, 0.01, 20).tolist() + np.linspace(0.01, 0.03, 10).tolist() \n",
    "\n",
    "## 2B \n",
    "alpha_inc_2B_2_5= np.linspace(0.0007, 0.002, 20).tolist() + np.linspace(0.002, 0.01, 20).tolist() \n",
    "\n",
    "alpha_values=alpha_vgg_1C\n",
    "\n",
    "\n",
    "classifiers=['RandomForest', 'Logistic', 'XgBoost',  'SVM', 'ensemble', 'MLP']\n",
    "classifiers=['Logistic']\n",
    "selectors=['lasso', 'rf', 'mrmr', 'logistic']\n",
    "selectors=['rf']\n",
    "modes=['Max','Mean','MV']\n",
    "modes=['MV']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\n",
    "               'fold'\n",
    "                'classifier': None,\n",
    "                'selector': None,\n",
    "                'alpha': None,\n",
    "                'num_features': None,\n",
    "                'pr_auc': None,\n",
    "                'best_precision': None,\n",
    "                'best_recall': None,\n",
    "                'roc_auc': None,\n",
    "                'f1': None,\n",
    "                'accuracy': None,\n",
    "                'confusion_matrix': [],\n",
    "                'selected_features': [],\n",
    "                'mode': None,\n",
    "                'balanced accuracy': None\n",
    "            }\n",
    "\n",
    "\n",
    "results_val_others = [template_dict.copy() for _ in range(50000)]\n",
    "results_val_others.append(template_dict.copy())\n",
    "\n",
    "results_val_lasso = [template_dict.copy() for _ in range(50000)]\n",
    "results_val_lasso.append(template_dict.copy())\n",
    "\n",
    "\n",
    "limit=30\n",
    "\n",
    "smote = SMOTE(random_state=10)\n",
    " \n",
    "k=0\n",
    "\n",
    "n_folds=5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with fold: 0\n",
      "Starting with mode: MV\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: rf\n",
      "Starting with fold: 1\n",
      "Starting with mode: MV\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: rf\n",
      "Starting with fold: 2\n",
      "Starting with mode: MV\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: rf\n",
      "Starting with fold: 3\n",
      "Starting with mode: MV\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: rf\n",
      "Starting with fold: 4\n",
      "Starting with mode: MV\n",
      "Starting with classifier: Logistic\n",
      "Starting with selector: rf\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_index, val_index) in enumerate(skf.split(final_patients_train1, y_train1)):\n",
    "   \n",
    "    print(\"Starting with fold:\", fold_idx)\n",
    "\n",
    "    y_train, y_val = y_train1[train_index], y_train1[val_index]\n",
    "    patients_train, patients_val = patients_train1[train_index], patients_train1[val_index]\n",
    "    \n",
    "    final_patients_train = [final_patients_train1[i] for i in train_index]\n",
    "    final_patients_val = [final_patients_train1[i] for i in val_index]\n",
    "    \n",
    "    \n",
    "    x_train_expanded, y_train_expanded, _ = continue_array(final_patients_train, y_train)\n",
    "\n",
    "    x_train_expanded, y_train_expanded = smote.fit_resample(x_train_expanded, y_train_expanded)\n",
    "\n",
    "    for mode in modes:\n",
    "        print(\"Starting with mode:\", mode)\n",
    "        for i, classifier in enumerate(classifiers):\n",
    "                print(\"Starting with classifier:\", classifier)\n",
    "                for j, selector in enumerate(selectors):\n",
    "                    print(\"Starting with selector:\", selector)\n",
    "\n",
    "                    if(selector=='lasso'):\n",
    "\n",
    "                        for alpha in alpha_values:\n",
    "                        \n",
    "                            classi= classifierinitialization(classifier)\n",
    "                            \n",
    "                            best_case_val = classification_method(\n",
    "                                   selector, alpha, classi, x_train_expanded, y_train_expanded, \n",
    "                                   patients_val, y_val, final_patients_val, 0, mode, thresholds=0.5\n",
    "                              )\n",
    "                            \n",
    "                            #print(f\"for {alpha} found {best_case_val['num_features']} \")\n",
    "                             \n",
    "                            if((best_case_val==0) or  (best_case_val['num_features']> limit)):\n",
    "                                best_case_val=template_dict\n",
    "                                best_case_val['balanced accuracy'] = 0\n",
    "                                best_case_val['accuracy'] = 0\n",
    "                                best_case_val['f1'] = 0\n",
    "                                best_case_val['roc_auc'] = 0\n",
    "                                best_case_val['pr_auc'] = 0\n",
    "                                best_case_val['selected_features'] = [0]\n",
    "                                best_case_val['num_features'] = 0\n",
    "                                best_case_val['confusion_matrix'] = 0\n",
    "                            \n",
    "\n",
    "                            #print(f\"for {alpha} found {best_case_val['num_features']} \")\n",
    "                            \n",
    "                            results_val_lasso[k] = {\n",
    "                                                'fold': fold_idx,\n",
    "                                                'classifier': classifier,\n",
    "                                                'selector': selector,\n",
    "                                                'alpha': alpha,\n",
    "                                                'mode': mode,\n",
    "                                                'num_features': best_case_val['num_features'],\n",
    "                                                'selected_features': best_case_val['selected_features'],\n",
    "                                                'pr_auc': best_case_val['pr_auc'],\n",
    "                                                'roc_auc': best_case_val['roc_auc'],\n",
    "                                                'f1': best_case_val['f1'],\n",
    "                                                'accuracy': best_case_val['accuracy'],\n",
    "                                                'confusion_matrix': best_case_val['confusion_matrix'],\n",
    "                                                'balanced accuracy': best_case_val['balanced accuracy'],\n",
    "                                                }\n",
    "\n",
    "                            k = k + 1\n",
    "\n",
    "                    else:\n",
    "                        limit=30\n",
    "                        for t in range(2, limit):\n",
    "                                classi= classifierinitialization(classifier)\n",
    "\n",
    "                                best_case_val = classification_method(\n",
    "                                   selector, 0, classi, x_train_expanded, y_train_expanded, \n",
    "                                   patients_val, y_val, final_patients_val, t, mode, thresholds=0.5\n",
    "                              )\n",
    "                        \n",
    "                                    \n",
    "                                results_val_others[k] = {\n",
    "                                                'fold': fold_idx,\n",
    "                                                'classifier': classifier,\n",
    "                                                'selector': selector,\n",
    "                                                'alpha': 0,\n",
    "                                                'mode': mode,\n",
    "                                                'num_features': t,\n",
    "                                                'selected_features': best_case_val['selected_features'],\n",
    "                                                'pr_auc': best_case_val['pr_auc'],\n",
    "                                                'roc_auc': best_case_val['roc_auc'],\n",
    "                                                'f1': best_case_val['f1'],\n",
    "                                                'accuracy': best_case_val['accuracy'],\n",
    "                                                'confusion_matrix': best_case_val['confusion_matrix'],\n",
    "                                                'balanced accuracy': best_case_val['balanced accuracy'],\n",
    "                                }\n",
    "\n",
    "                                k = k + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinazioni migliori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered results_val_lasso: 0 entries remaining\n",
      "Filtered results_val_others: 140 entries remaining\n",
      "Migliori 1 combinazioni di parametri:\n",
      "\n",
      "#1:\n",
      "Classifier: Logistic\n",
      "Selector: rf\n",
      "Num_features/alpha: 7\n",
      "Mode : MV\n",
      "Performance medie sul val set: \n",
      "ROC AUC = 0.6734876543209877 (std = 0.06799073436586646), Balanced Accuracy = 0.6522222222222223 (std = 0.04981253436884045)\n",
      "[19 72  7  9 54 61]\n",
      " Mantenendo le features in questo modo: Features in at least 3 folds\n",
      " ha selezionato TROVATO DAL LOOP: [19 72  7  9 54 61] e sono 6\n",
      "Logistic, rf, 7, MV\n",
      " Balanced Accuracy: 0.7685185185185186\n",
      " Accuracy: 0.7435897435897436\n",
      " F1: 0.6666666666666666\n",
      " roc_auc: 0.7685185185185185\n",
      " confusion matrix: [[19  8]\n",
      " [ 2 10]]\n",
      "\n",
      "Best mode is  Features in at least 3 folds\n"
     ]
    }
   ],
   "source": [
    "#results_test_lasso = [entry for entry in results_test_lasso if entry['classifier'] is not None]\n",
    "#print(f\"Filtered results_test_lasso: {len(results_test_lasso)} entries remaining\")\n",
    "results_val_lasso= [entry for entry in results_val_lasso if entry['selector'] is not None]\n",
    "print(f\"Filtered results_val_lasso: {len(results_val_lasso)} entries remaining\")\n",
    "#results_test_others = [entry for entry in results_test_others if entry['classifier'] is not None]\n",
    "#print(f\"Filtered results_test_others: {len(results_test_others)} entries remaining\")\n",
    "results_val_others = [entry for entry in results_val_others if entry['selector'] is not None]\n",
    "print(f\"Filtered results_val_others: {len(results_val_others)} entries remaining\")\n",
    "\n",
    "import statistics\n",
    "\n",
    "num_features_range = list(range(2, 30))\n",
    "\n",
    "grid_results_others = {}\n",
    "grid_results_lasso = {}\n",
    "\n",
    "selectors = ['mrmr', 'rf', 'logistic']\n",
    "\n",
    "# Itera su tutte le combinazioni di parametri (classifier, selector, num_features, threshold)\n",
    "for classifier in classifiers:\n",
    "    #print(f\"Sto iniziando classifier {classifier}\")\n",
    "    for selector in selectors:\n",
    "            for mode in modes:\n",
    "            #print(f\"Sto iniziando selector {selector}\")\n",
    "                for num_features in num_features_range:\n",
    "                        \n",
    "                        # Filtra i risultati che corrispondono a questa combinazione di parametri\n",
    "                        filtered_results=[]\n",
    "                        for res in results_val_others:\n",
    "                            ## qui filtro per num_features\n",
    "                            if (res['classifier'] == classifier and res['selector'] == selector and res['num_features'] == num_features and res['mode']==mode):\n",
    "                                filtered_results.append(res)\n",
    "                    \n",
    "                        if filtered_results:\n",
    "                            f1_values = [res['f1'] for res in filtered_results]\n",
    "                            balaccuracy_values = [res['balanced accuracy'] for res in filtered_results]\n",
    "                            roc_values=[res['roc_auc'] for res in filtered_results]\n",
    "\n",
    "                            # Calcola le medie delle metriche\n",
    "                            avg_f1 = sum(f1_values) / len(f1_values)\n",
    "                            avg_balaccuracy = sum(balaccuracy_values) / len(balaccuracy_values)\n",
    "                            avg_roc = sum(roc_values) / len(roc_values)\n",
    "\n",
    "                            # Calcola la deviazione standard delle metriche\n",
    "                            std_f1 = statistics.stdev(f1_values) if len(f1_values) > 1 else 0\n",
    "                            std_balaccuracy = statistics.stdev(balaccuracy_values) if len(balaccuracy_values) > 1 else 0\n",
    "                            std_roc_auc = statistics.stdev(roc_values) if len(roc_values) > 1 else 0\n",
    "\n",
    "                            # Memorizza i risultati medi e la deviazione standard di questa combinazione\n",
    "                            grid_results_others[(classifier, selector, num_features, mode)] = {\n",
    "                                'avg_f1': avg_f1,\n",
    "                                'std_f1': std_f1,\n",
    "                                'avg_balaccuracy': avg_balaccuracy,\n",
    "                                'std_balaccuracy': std_balaccuracy,\n",
    "                                'avg_roc_auc': avg_roc,\n",
    "                                'std_roc_auc': std_roc_auc\n",
    "                            }\n",
    "\n",
    "\n",
    "\n",
    "## ORA PER LASSO\n",
    "selectors = ['lasso']\n",
    "for classifier in classifiers:\n",
    "    #print(f\"Sto iniziando classifier {classifier}\")\n",
    "    for selector in selectors:\n",
    "        for mode in modes:\n",
    "            #print(f\"Sto iniziando selector {selector}\")\n",
    "            for alpha in alpha_values:\n",
    "                    filtered_results = []\n",
    "                    for res in results_val_lasso:\n",
    "                        ## qui filtro per alpha\n",
    "                        if (res['classifier'] == classifier and res['selector'] == selector and res['alpha'] == alpha and res['mode']==mode):\n",
    "                            filtered_results.append(res)\n",
    "\n",
    "                    if filtered_results:\n",
    "                            f1_values = [res['f1'] for res in filtered_results]\n",
    "                            balaccuracy_values = [res['balanced accuracy'] for res in filtered_results]\n",
    "                            roc_values=[res['roc_auc'] for res in filtered_results]\n",
    "\n",
    "                            # Calcola le medie delle metriche\n",
    "                            avg_f1 = sum(f1_values) / len(f1_values)\n",
    "                            avg_balaccuracy = sum(balaccuracy_values) / len(balaccuracy_values)\n",
    "                            avg_roc = sum(roc_values) / len(roc_values)\n",
    "\n",
    "                            # Calcola la deviazione standard delle metriche\n",
    "                            std_f1 = statistics.stdev(f1_values) if len(f1_values) > 1 else 0\n",
    "                            std_balaccuracy = statistics.stdev(balaccuracy_values) if len(balaccuracy_values) > 1 else 0\n",
    "                            std_roc_auc = statistics.stdev(roc_values) if len(roc_values) > 1 else 0\n",
    "\n",
    "                            # Memorizza i risultati medi e la deviazione standard di questa combinazione\n",
    "                            grid_results_lasso[(classifier, selector, alpha, mode)] = {\n",
    "                                'avg_f1': avg_f1,\n",
    "                                'std_f1': std_f1,\n",
    "                                'avg_balaccuracy': avg_balaccuracy,\n",
    "                                'std_balaccuracy': std_balaccuracy,\n",
    "                                'avg_roc_auc': avg_roc,\n",
    "                                'std_roc_auc': std_roc_auc,\n",
    "                            }\n",
    "\n",
    "\n",
    "# Ordina le combinazioni per 'avg_f1', e in caso di parità, per 'avg_pr_auc'\n",
    "sorted_results_others = sorted(grid_results_others.items(), key=lambda x: (x[1]['avg_balaccuracy'], x[1]['avg_roc_auc']),reverse=True)\n",
    "sorted_results_lasso = sorted(grid_results_lasso.items(), key=lambda x: (x[1]['avg_balaccuracy'], x[1]['avg_roc_auc']), reverse=True)\n",
    "\n",
    "#sorted_results_others = sorted(grid_results_others.items(), key=lambda x: (x[1]['avg_roc_auc'], x[1]['avg_balaccuracy']),reverse=True)\n",
    "#sorted_results_lasso = sorted(grid_results_lasso.items(), key=lambda x: (x[1]['avg_roc_auc'], x[1]['avg_balaccuracy']), reverse=True)\n",
    "\n",
    "import functools\n",
    "\n",
    "# Funzione di ordinamento personalizzata\n",
    "def compare_items(item1, item2):\n",
    "    balacc1 = item1[1]['avg_balaccuracy']\n",
    "    balacc2 = item2[1]['avg_balaccuracy']\n",
    "    \n",
    "    # Se la differenza tra le balanced accuracies è minore di 0.001, confronta la ROC AUC\n",
    "    if abs(balacc1 - balacc2) < 0.003:\n",
    "        roc_auc1 = item1[1]['avg_roc_auc']\n",
    "        roc_auc2 = item2[1]['avg_roc_auc']\n",
    "        # Confronta la ROC AUC e ritorna -1, 0 o 1 per l'ordinamento\n",
    "        if roc_auc1 > roc_auc2:\n",
    "            return 1\n",
    "        elif roc_auc1 < roc_auc2:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        # Altrimenti ordina per balanced accuracy\n",
    "        if balacc1 > balacc2:\n",
    "            return 1\n",
    "        elif balacc1 < balacc2:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "# Combina i risultati di others e lasso\n",
    "sorted_results = sorted_results_others + sorted_results_lasso\n",
    "\n",
    "# Utilizza cmp_to_key per usare la funzione di comparazione personalizzata\n",
    "sorted_results = sorted(sorted_results, key=functools.cmp_to_key(compare_items), reverse=True)\n",
    "\n",
    "n=1\n",
    "best_combinations = sorted_results[:n] ## mostrando le n migliori configurazioni\n",
    "\n",
    "print(f\"Migliori {n} combinazioni di parametri:\")\n",
    "for i, (params, metrics) in enumerate(best_combinations, start=1):\n",
    "\n",
    "    classi= classifierinitialization(params[0])\n",
    "    if(params[1]=='lasso'):\n",
    "        best_case = classification_method(\n",
    "                                    params[1], params[2], classi, x_train_expanded1, y_train_expanded1, \n",
    "                                    patients_test, y_test, final_patients_test, 0, params[3], thresholds=0.5\n",
    "                                )\n",
    "    \n",
    "    else:\n",
    "        best_case = classification_method(\n",
    "                                    params[1], 0, classi, x_train_expanded1, y_train_expanded1, \n",
    "                                    patients_test, y_test, final_patients_test, params[2] , params[3], thresholds=0.5\n",
    "                                )\n",
    "\n",
    "    print(f\"\\n#{i}:\")\n",
    "    print(f\"Classifier: {params[0]}\")\n",
    "    print(f\"Selector: {params[1]}\")\n",
    "    print(f\"Num_features/alpha: {params[2]}\")\n",
    "    print(f\"Mode : {params[3]}\")\n",
    "\n",
    "    print(f\"Performance medie sul val set: \\nROC AUC = {metrics['avg_roc_auc']} (std = {metrics['std_roc_auc']}), \"f\"Balanced Accuracy = {metrics['avg_balaccuracy']} (std = {metrics['std_balaccuracy']})\")\n",
    "\n",
    "    if (i==1):\n",
    "           best_classifier1=params[0]\n",
    "           best_selector1=params[1]\n",
    "           best_param1=params[2]\n",
    "           best_mode1=params[3]\n",
    "\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "\n",
    "best_classifier=best_classifier1\n",
    "best_selector=best_selector1\n",
    "best_num_features=best_param1\n",
    "best_mode_fold=best_mode1\n",
    "\n",
    "\n",
    "modes_selection=['Only the features in every fold', 'All the features', 'Features in at least 3 folds', 'Features in the best fold']\n",
    "modes_selection=['All the features', 'Features in at least 3 folds', 'Features in the best fold']\n",
    "modes_selection=['All the features', 'Features in the best fold']\n",
    "modes_selection=['Features in at least 3 folds']\n",
    "\n",
    "best_fold=None\n",
    "best_mode=None\n",
    "best_ba=0\n",
    "\n",
    "balanced_acc=0\n",
    "selected_features_array=[]\n",
    "\n",
    "if (best_selector == 'lasso'):\n",
    "    for res in results_val_lasso:\n",
    "            if (res['classifier']==best_classifier and res['selector']==best_selector and res['alpha']==best_num_features and res['mode']==best_mode_fold):\n",
    "                    selected_features_array.append(res['selected_features'])\n",
    "                    if(res['balanced accuracy']>balanced_acc):\n",
    "                        balanced_acc = res['balanced accuracy']\n",
    "\n",
    "                        best_fold=res['fold']\n",
    "else:\n",
    "    for res in results_val_others:\n",
    "        if (res['classifier']==best_classifier and res['selector']==best_selector and res['num_features']==best_num_features and res['mode']==best_mode_fold):\n",
    "            selected_features_array.append(res['selected_features'])\n",
    "            #print(res)\n",
    "            if(res['balanced accuracy']>balanced_acc):\n",
    "                balanced_acc = res['balanced accuracy']\n",
    "                best_fold=res['fold']\n",
    "                \n",
    "\n",
    "#print(selected_features_array)\n",
    "for mode in modes_selection: \n",
    "    if (mode=='Only the features in every fold'):\n",
    "        selected_features = reduce(np.intersect1d, selected_features_array)\n",
    "\n",
    "    if(mode=='All the features'):\n",
    "        selected_features = np.unique(np.concatenate(selected_features_array))\n",
    "\n",
    "    if(mode=='Features in at least 3 folds'):\n",
    "        all_elements = np.concatenate(selected_features_array)  # Unisci tutti gli array in uno\n",
    "        element_counts = Counter(all_elements)  # Conta le occorrenze di ciascun elemento\n",
    "        selected_features = np.array([key for key, count in element_counts.items() if count >= 3])\n",
    "        print(selected_features)\n",
    "\n",
    "    if(mode=='Features in the best fold'):\n",
    "        selected_features = selected_features_array[best_fold]\n",
    "\n",
    "    classi=classifierinitialization(best_classifier)\n",
    "    best_case_test= classification_method(\n",
    "                                        best_selector, 0, classi, x_train_expanded1, y_train_expanded1, \n",
    "                                        patients_test, y_test, final_patients_test, best_num_features, best_mode_fold, thresholds=0.5, mode='features', selected_features=selected_features\n",
    "                                    )\n",
    "\n",
    "    print(f\" Mantenendo le features in questo modo: {mode}\\n ha selezionato TROVATO DAL LOOP: {selected_features} e sono {len(selected_features)}\")\n",
    "    print(f\"{best_classifier}, {best_selector}, {best_num_features}, {best_mode_fold}\")\n",
    "    print(f\" Balanced Accuracy: {best_case_test['balanced accuracy']}\")\n",
    "    print(f\" Accuracy: {best_case_test['accuracy']}\")\n",
    "    print(f\" F1: {best_case_test['f1']}\")\n",
    "    print(f\" roc_auc: {best_case_test['roc_auc']}\")\n",
    "    print(f\" confusion matrix: {best_case_test['confusion_matrix']}\\n\")\n",
    "\n",
    "    if(best_ba<best_case_test['balanced accuracy']):\n",
    "        best_ba=best_case_test['balanced accuracy']\n",
    "        best_mode=mode\n",
    "\n",
    "print(\"Best mode is \", best_mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Combinati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "CSV salvato in: \\Users\\bsbar\\Desktop\\selected_features_25_VGG.csv\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "    with z.open(z.namelist()[0]) as f:  # Assumendo che ci sia solo un file nel zip\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "# Ordina per la colonna 'Paziente' (che è la prima colonna)\n",
    "df_sorted = df.sort_values(by=df.columns[0])  # 'Paziente' è la prima colonna\n",
    "\n",
    "# Seleziona le colonne specificate da original_features\n",
    "# Aggiungi 2 a original_features per escludere le prime due colonne (Paziente e Slice)\n",
    "final_features=original_features[selected_features]\n",
    "\n",
    "print(len(original_features))\n",
    "selected_columns = [0, 1] + [index + 2 for index in final_features]  # Aggiungi gli indici di original_features\n",
    "filtered_df = df_sorted.iloc[:, selected_columns]\n",
    "\n",
    "# Carica l'array delle tuple\n",
    "file_path = \"../indici_aree/aree_07.npy\"\n",
    "arraytuple = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "# Converti il primo elemento di ogni tupla in int\n",
    "arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "# Raggruppa le tuple per paziente\n",
    "patients_slices = {}\n",
    "for t in arraytuple:\n",
    "    patient_id = t[0]\n",
    "    if patient_id not in patients_slices:\n",
    "        patients_slices[patient_id] = []\n",
    "    patients_slices[patient_id].append(t)\n",
    "\n",
    "# Per ogni paziente, tieni solo le 5 slice con area maggiore\n",
    "for patient_id in patients_slices:\n",
    "    patients_slices[patient_id] = sorted(patients_slices[patient_id], key=lambda x: x[2], reverse=True)[:5]\n",
    "\n",
    "final_tuples = []\n",
    "for patient_id in sorted(patients_slices):\n",
    "    final_tuples.extend(patients_slices[patient_id])\n",
    "\n",
    "# Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di final_tuples\n",
    "filtered_data = filtered_df[\n",
    "    filtered_df.apply(lambda row: any((row['Patient'] == (t[0]) and row['Slice'] == t[1]) for t in final_tuples), axis=1)\n",
    "]\n",
    "\n",
    "# Salva il DataFrame risultante in un CSV\n",
    "output_path = r\"\\Users\\bsbar\\Desktop\\selected_features_25_VGG.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV salvato in: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radiomica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "CSV salvato in: \\Users\\bsbar\\Desktop\\selected_features_25_Radiomica.csv\n"
     ]
    }
   ],
   "source": [
    "zip_file_path = \"../CSV/GMP/EncodersAllSlices/Radiomica_Wavelet_25D.csv.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "    with z.open(z.namelist()[0]) as f:  # Assumendo che ci sia solo un file nel zip\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "df_cleaned = df.loc[:, ~df.columns.str.contains('diagnostic', case=False)]\n",
    "\n",
    "final_features=original_features[selected_features]\n",
    "\n",
    "print(len(final_features))\n",
    "selected_columns = [0, 1] + [index + 2 for index in final_features]  # Aggiungi gli indici di original_features\n",
    "filtered_df = df_cleaned.iloc[:, selected_columns]\n",
    "\n",
    "# Carica l'array delle tuple\n",
    "file_path = \"../indici_aree/aree_07.npy\"\n",
    "arraytuple = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "# Converti il primo elemento di ogni tupla in int\n",
    "arraytuple = [(int(t[0]), t[1], t[2]) for t in arraytuple]\n",
    "\n",
    "# Raggruppa le tuple per paziente\n",
    "patients_slices = {}\n",
    "for t in arraytuple:\n",
    "    patient_id = t[0]\n",
    "    if patient_id not in patients_slices:\n",
    "        patients_slices[patient_id] = []\n",
    "    patients_slices[patient_id].append(t)\n",
    "\n",
    "# Per ogni paziente, tieni solo le 5 slice con area maggiore\n",
    "for patient_id in patients_slices:\n",
    "    patients_slices[patient_id] = sorted(patients_slices[patient_id], key=lambda x: x[2], reverse=True)[:5]\n",
    "\n",
    "final_tuples = []\n",
    "for patient_id in sorted(patients_slices):\n",
    "    final_tuples.extend(patients_slices[patient_id])\n",
    "\n",
    "# Filtra il DataFrame per mantenere solo le righe che corrispondono agli elementi di final_tuples\n",
    "filtered_data = filtered_df[\n",
    "    filtered_df.apply(lambda row: any((row['Paziente'] == (t[0]) and row['Slice'] == t[1]) for t in final_tuples), axis=1)\n",
    "]\n",
    "\n",
    "output_path = r\"\\Users\\bsbar\\Desktop\\selected_features_25_Radiomica.csv\"\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"CSV salvato in: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne del primo DataFrame: ['Patient', 'Slice', 'Feature_494', 'Feature_435', 'Feature_305', 'Feature_74', 'Feature_213', 'Feature_398', 'Feature_39', 'Feature_472', 'Feature_355', 'Feature_249', 'Feature_319', 'Feature_308', 'Feature_176', 'Feature_493', 'Feature_32', 'Feature_474', 'Feature_137', 'Feature_29', 'Feature_117', 'Feature_452', 'Feature_349']\n",
      "Colonne del secondo DataFrame: ['Patient', 'Slice', 'wavelet-LH_firstorder_Uniformity', 'wavelet-LH_glcm_Correlation', 'wavelet-HL_gldm_DependenceVariance', 'wavelet-LH_gldm_DependenceVariance', 'original_shape2D_MinorAxisLength', 'original_firstorder_10Percentile']\n",
      "Il file CSV unito è stato salvato come 'merged_file.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Percorsi dei file CSV\n",
    "csv_file_1 = r\"\\Users\\bsbar\\Desktop\\selected_features_25_VGG.csv\"\n",
    "csv_file_2 = r\"\\Users\\bsbar\\Desktop\\selected_features_25_Radiomica.csv\"  \n",
    "\n",
    "# Carica i CSV in DataFrame\n",
    "df1 = pd.read_csv(csv_file_1)\n",
    "df2 = pd.read_csv(csv_file_2)\n",
    "\n",
    "# Rimuovi spazi dai nomi delle colonne\n",
    "df1.columns = df1.columns.str.strip()\n",
    "df2.columns = df2.columns.str.strip()\n",
    "\n",
    "# Rinomina la colonna 'Paziente' in 'Patient' nel secondo DataFrame, se presente\n",
    "if 'Paziente' in df2.columns:\n",
    "    df2.rename(columns={'Paziente': 'Patient'}, inplace=True)\n",
    "\n",
    "# Stampa i nomi delle colonne per verifica\n",
    "print(\"Colonne del primo DataFrame:\", df1.columns.tolist())\n",
    "print(\"Colonne del secondo DataFrame:\", df2.columns.tolist())\n",
    "\n",
    "# Crea una lista per memorizzare le righe unite\n",
    "merged_rows = []\n",
    "\n",
    "# Itera sulle righe del primo DataFrame\n",
    "for index1, row1 in df1.iterrows():\n",
    "    found_match = False\n",
    "    \n",
    "    # Itera sulle righe del secondo DataFrame\n",
    "    for index2, row2 in df2.iterrows():\n",
    "        # Controlla se i primi due valori sono uguali\n",
    "        if row1[0] == row2[0] and row1[1] == row2[1]:  # Assumendo che le prime colonne siano in posizione 0 e 1\n",
    "            # Aggiungi le colonne del secondo DataFrame (eccetto le prime due) alla riga del primo DataFrame\n",
    "            new_row = row1.tolist() + row2[2:].tolist()  # Unisce le righe\n",
    "            merged_rows.append(new_row)\n",
    "            found_match = True\n",
    "            break  # Esci dal ciclo interno se trovi un match\n",
    "    \n",
    "    # Se non trovi un match, puoi decidere se aggiungere la riga originale del primo DataFrame o meno\n",
    "    if not found_match:\n",
    "        merged_rows.append(row1.tolist())  # Aggiungi solo la riga del primo DataFrame\n",
    "\n",
    "# Crea un nuovo DataFrame con le righe unite\n",
    "merged_df = pd.DataFrame(merged_rows, columns=list(df1.columns) + list(df2.columns[2:]))\n",
    "\n",
    "# Salva il DataFrame risultante in un nuovo CSV\n",
    "merged_df.to_csv('merged_file.csv', index=False)\n",
    "\n",
    "print(\"Il file CSV unito è stato salvato come 'merged_file.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
