{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_recall_curve, balanced_accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  # Needed for NaN check\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from scipy.stats import ttest_ind\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, f1_score, balanced_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0\n",
      " 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0\n",
      " 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0\n",
      " 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1]\n",
      "Number of labels: 129\n",
      "Patient Names:  [5, 12, 15, 16, 17, 19, 22, 23, 24, 25, 26, 27, 29, 30, 31, 33, 35, 36, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 50, 52, 53, 54, 56, 57, 58, 59, 60, 61, 62, 64, 65, 68, 69, 70, 71, 74, 75, 76, 78, 79, 81, 82, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 98, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 123, 124, 126, 127, 128, 129, 133, 135, 136, 137, 138, 139, 141, 142, 144, 146, 147, 149, 150, 153, 155, 158, 159, 161, 163, 166, 168, 169, 170, 171, 175, 176, 178, 182, 183, 188, 189, 190, 193, 197, 199, 200, 205]\n",
      "All slices with NaN values have been removed.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ThesisPlaques\\\\CSV\\\\data_rad_clin_DEF.csv\"\n",
    "\n",
    "data = pd.read_csv(file_path)\n",
    "labels_column = data['label']\n",
    "labels = labels_column.astype(int).tolist()\n",
    "\n",
    "labels=np.array(labels)\n",
    "\n",
    "# Estrazione dei numeri dai nomi dei pazienti\n",
    "loaded_patients = data['IDs_new'].str.extract(r'(\\d+)').astype(int).squeeze().tolist()\n",
    "\n",
    "print(\"Labels:\", labels)\n",
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Patient Names: \", loaded_patients )\n",
    "\n",
    "# Load the data\n",
    "file_path = \"C:\\\\Users\\\\bsbar\\\\Desktop\\\\Radiomica_2_5D.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Filter columns that start with 'original'\n",
    "filtered_columns = [col for col in data.columns if col.startswith('original')]\n",
    "\n",
    "\n",
    "\n",
    "patients = []\n",
    "\n",
    "for patient_id in loaded_patients:\n",
    "\n",
    "    # Filter the data for the specific patient\n",
    "    patient_data = data[data['Paziente'] == patient_id]\n",
    "    \n",
    "    slices = []\n",
    "    \n",
    "    for _, slice_row in patient_data.iterrows():\n",
    "        # Select only the filtered columns for each slice\n",
    "        slice_features = slice_row[filtered_columns].tolist()\n",
    "        \n",
    "        # Check for NaN values in the slice, and only append if there are no NaNs\n",
    "        if not any(np.isnan(value) for value in slice_features):\n",
    "            slices.append(slice_features)\n",
    "    \n",
    "    patients.append(slices)\n",
    "\n",
    "# Optional: Check if all slices with NaN were removed successfully\n",
    "for i, patient in enumerate(patients):\n",
    "    for j, slice_features in enumerate(patient):\n",
    "        assert not any(np.isnan(value) for value in slice_features), f\"NaN found in patient {i}, slice {j}\"\n",
    "\n",
    "print(\"All slices with NaN values have been removed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_array(filtered_patients, labels):\n",
    "    all_features = []\n",
    "    for patient in filtered_patients:\n",
    "        for image_features in patient:\n",
    "            all_features.append(image_features)\n",
    "\n",
    "    all_features_array = np.array(all_features)\n",
    "    expanded_labels = []\n",
    "    expanded_patient_ids = []\n",
    "\n",
    "    for i in range(len(filtered_patients)):\n",
    "        num_images = len(filtered_patients[i])\n",
    "        expanded_labels.extend([labels[i]] * num_images)\n",
    "        expanded_patient_ids.extend([loaded_patients[i]] * num_images)\n",
    "\n",
    "    expanded_labels_array = np.array(expanded_labels)\n",
    "    expanded_patient_ids_array = np.array(expanded_patient_ids)\n",
    "\n",
    "    return all_features_array, expanded_labels_array, expanded_patient_ids_array\n",
    "\n",
    "\n",
    "## funzioni per feature correlation\n",
    "def filter_highly_correlated_features(df, corr, threshold=0.85):\n",
    "    columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "    removed_features = []\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= threshold:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                    removed_features.append(df.columns[j])\n",
    "\n",
    "    return removed_features\n",
    "\n",
    "\n",
    "def perform_correlation(z_train, y_train, numero = 32, threshold = 0.85):\n",
    "    all_images, _, _= continue_array(z_train, y_train)\n",
    "\n",
    "    df = pd.DataFrame(all_images, columns=[f'feature_{i}' for i in range(numero)])\n",
    "\n",
    "    corr_matrix = df.corr()\n",
    "\n",
    "    features_selected = filter_highly_correlated_features(df, corr_matrix, threshold)\n",
    "    \n",
    "    return features_selected\n",
    "\n",
    "def select_features_by_p_value(x_train_expanded, y_train_expanded, p_value_threshold=0.05):\n",
    "\n",
    "    p_values = []\n",
    "    num_features = x_train_expanded.shape[1]\n",
    "\n",
    "    for i in range(num_features):\n",
    "        feature = x_train_expanded[:, i]\n",
    "        group_0 = feature[y_train_expanded == 0]\n",
    "        group_1 = feature[y_train_expanded == 1]\n",
    "        t_stat, p_val = ttest_ind(group_0, group_1, equal_var=False)\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    p_values = np.array(p_values)\n",
    "\n",
    "    selected_features_indices = np.where(p_values < p_value_threshold)[0]\n",
    "\n",
    "    sorted_indices = selected_features_indices[np.argsort(p_values[selected_features_indices])]\n",
    "\n",
    "    x_train_expanded = x_train_expanded[:, sorted_indices]\n",
    "\n",
    "    return x_train_expanded, sorted_indices\n",
    "\n",
    "\n",
    "\n",
    "## funzione per rimozione di features specifiche\n",
    "def remove_features_from_patients(patients, features_to_remove):\n",
    "    feature_indices_to_remove = [int(feature.split('_')[1]) for feature in features_to_remove]\n",
    "    \n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.delete(image_features, feature_indices_to_remove, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))    \n",
    "\n",
    "    return final_patients\n",
    "\n",
    "## funzione per lasciare solo le features indicate per array di array\n",
    "def keep_features_in_patients(patients, features_to_keep):\n",
    "\n",
    "    feature_indices_to_keep = [int(feature) for feature in features_to_keep]\n",
    "\n",
    "    final_patients = []\n",
    "    for patient in patients:\n",
    "        new_patients = []\n",
    "        for image_features in patient:\n",
    "            new_patient = np.take(image_features, feature_indices_to_keep, axis=0)\n",
    "            new_patients.append(new_patient)\n",
    "        final_patients.append(np.array(new_patients))\n",
    "\n",
    "    return final_patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train patients:  63\n",
      "Number of test patients:  39\n",
      "Number of val patients:  27\n",
      "Number of features for every image:  102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# patients_train contiene il nome dei pazienti (5,12 etc)\n",
    "# y_train contiene le labels\n",
    "# features_train contiene array di array dove ogni paziente ha varie immagini rappresentate da n features\n",
    "\n",
    "patients_train1, patients_test, y_train1, y_test, features_train1, features_test= train_test_split(loaded_patients, labels, patients, test_size=0.3, shuffle=False, random_state=1)\n",
    "patients_train, patients_val, y_train, y_val, features_train, features_val= train_test_split(patients_train1, y_train1, features_train1, test_size=0.3, shuffle=True, stratify=y_train1, random_state=3)\n",
    "#patients_train, patients_val, y_train, y_val, features_train, features_val= train_test_split(patients_train1, y_train1, features_train1, test_size=0.3, shuffle=False, random_state=1)\n",
    "\n",
    "print(\"Number of train patients: \", len(features_train))\n",
    "print(\"Number of test patients: \", len(features_test))\n",
    "print(\"Number of val patients: \", len(features_val))\n",
    "\n",
    "print(\"Number of features for every image: \", len(features_train[0][0]) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 24)\n",
      "x_train_expanded (2553, 11)\n",
      "final_patients_val (39, 11)\n",
      "final_patients_test (41, 11)\n",
      "final_patients_train1 (39, 11)\n",
      "Scelte 11 features\n",
      "\n",
      "x_train_expanded1 (3517, 11)\n"
     ]
    }
   ],
   "source": [
    "starting_features = len(features_train[0][0])\n",
    "features=perform_correlation(features_train, y_train, starting_features, 0.8)\n",
    "\n",
    "final_patients_train=remove_features_from_patients(features_train, features)\n",
    "final_patients_test=remove_features_from_patients(features_test, features)\n",
    "final_patients_val=remove_features_from_patients(features_val, features)\n",
    "final_patients_train1=remove_features_from_patients(features_train1, features)\n",
    "print(final_patients_train1[0].shape)\n",
    "x_train_expanded, y_train_expanded, _ = continue_array(final_patients_train, y_train)\n",
    "x_train_expanded, sf= select_features_by_p_value(x_train_expanded, y_train_expanded, 0.01)\n",
    "print(\"x_train_expanded\", x_train_expanded.shape)\n",
    "\n",
    "\n",
    "final_patients_test=keep_features_in_patients(final_patients_test, sf)\n",
    "final_patients_val=keep_features_in_patients(final_patients_val, sf)\n",
    "final_patients_train1=keep_features_in_patients(final_patients_train1, sf)\n",
    "print(\"final_patients_val\", final_patients_val[0].shape)\n",
    "print(\"final_patients_test\", final_patients_test[0].shape)\n",
    "print(\"final_patients_train1\", final_patients_train1[0].shape)\n",
    "print(f\"Scelte {len(sf)} features\\n\")\n",
    "\n",
    "\n",
    "x_train_expanded1, y_train_expanded1, _ = continue_array(final_patients_train1, y_train1)\n",
    "print(\"x_train_expanded1\", x_train_expanded1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RICERCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_with_voting(y_true, predictions_slices, probabilities_slices, voting_type='majority'):\n",
    "    if voting_type == 'majority':\n",
    "        y_pred_final = majority_voting(predictions_slices)\n",
    "    elif voting_type == 'mean':\n",
    "        y_pred_final = mean_voting(probabilities_slices)\n",
    "    else:\n",
    "        raise ValueError(\"Voting type not supported.\")\n",
    "    \n",
    "    # Calcolo della probabilità media per ogni paziente per la classe positiva\n",
    "    y_prob_final = np.array([np.mean(p[:, 1]) for p in probabilities_slices])\n",
    "    \n",
    "    # Calcolo delle metriche a livello di paziente\n",
    "    f1 = f1_score(y_true, y_pred_final)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob_final)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob_final)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred_final)\n",
    "    \n",
    "    return f1, roc_auc, pr_auc, balanced_acc\n",
    "\n",
    "# Classificatori supportati\n",
    "def get_classifier(name, params):\n",
    "    if name == 'random_forest':\n",
    "        return RandomForestClassifier(**params, random_state=42)\n",
    "    elif name == 'logistic':\n",
    "        return LogisticRegression(**params, random_state=42)\n",
    "    elif name == 'mlp':\n",
    "        return MLPClassifier(**params, random_state=42)\n",
    "    elif name == 'xgboost':\n",
    "        return XGBClassifier(**params,random_state=42)\n",
    "    elif name == 'svm':\n",
    "        return SVC(probability=True, **params, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(f\"Classifier {name} not supported.\")\n",
    "    \n",
    "def majority_voting(predictions):\n",
    "    return np.array([np.bincount(pred).argmax() for pred in predictions])\n",
    "\n",
    "def mean_voting(predictions_prob):\n",
    "    return np.array([np.mean(pred, axis=0).argmax() for pred in predictions_prob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_classifier(X_train, y_train, X_val, y_val, classifier_name, param_grid):\n",
    "    # Usa la funzione continue_array per trasformare l'array di validation nel formato di train\n",
    "    X_val_slices, y_val_slices,_ = continue_array(X_val, y_val)\n",
    "    \n",
    "    # Unisci train e validation\n",
    "    X_train_combined = np.vstack([X_train, X_val_slices])\n",
    "    y_train_combined = np.hstack([y_train, y_val_slices])\n",
    "    \n",
    "    # Definizione del classificatore\n",
    "    classifier = get_classifier(classifier_name, {})\n",
    "    \n",
    "    # Ricerca dei migliori parametri con GridSearchCV\n",
    "    grid_search = GridSearchCV(classifier, param_grid, cv=3, scoring='f1', verbose=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train_combined, y_train_combined)\n",
    "    \n",
    "    # Ritorna il miglior classificatore\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    print(f\"Best params for {classifier_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    return best_clf, grid_search.best_params_\n",
    "\n",
    "def test_with_voting(classifier, X_data, y_data, voting_type='majority'):\n",
    "    # Predizioni e probabilità per ogni paziente\n",
    "    predictions_slices = []\n",
    "    probabilities_slices = []\n",
    "    \n",
    "    for patient_slices in X_data:\n",
    "        patient_pred = classifier.predict(patient_slices)\n",
    "        patient_prob = classifier.predict_proba(patient_slices)\n",
    "        predictions_slices.append(patient_pred)\n",
    "        probabilities_slices.append(patient_prob)\n",
    "    \n",
    "    # Applica majority o mean voting e calcola le metriche\n",
    "    f1, roc_auc, pr_auc, balanced_acc = calculate_metrics_with_voting(y_data, predictions_slices, probabilities_slices, voting_type)\n",
    "    \n",
    "    print(f\"Metrics with {voting_type} voting: F1 = {f1}, ROC AUC = {roc_auc}, PR AUC = {pr_auc}, Balanced Accuracy = {balanced_acc}\")\n",
    "    \n",
    "    return f1, roc_auc, pr_auc, balanced_acc\n",
    "\n",
    "def retrain_and_test(classifier, X_train, y_train, X_val, y_val, X_test, y_test, voting_type='majority'):\n",
    "    # Usa la funzione continue_array per trasformare il validation set\n",
    "    X_val_slices, y_val_slices = continue_array(X_val, y_val)\n",
    "    \n",
    "    # Unire i dati di train e validation\n",
    "    X_train_combined = np.vstack([X_train, X_val_slices])\n",
    "    y_train_combined = np.hstack([y_train, y_val_slices])\n",
    "    \n",
    "    # Shuffle i dati uniti\n",
    "    X_train_combined, y_train_combined = shuffle(X_train_combined, y_train_combined, random_state=42)\n",
    "    \n",
    "    # Riallenamento sui dati combinati\n",
    "    classifier.fit(X_train_combined, y_train_combined)\n",
    "    \n",
    "    # Test sul test set\n",
    "    print(\"Test metrics:\")\n",
    "    test_with_voting(classifier, X_test, y_test, voting_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(X_train, y_train, X_val, y_val, X_test, y_test, classifier_name, voting_type='majority'):\n",
    "    # Parametri per i vari classificatori (come già definito in precedenza)\n",
    "    param_grid = {\n",
    "    'random_forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "    'logistic': [\n",
    "        {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear'],\n",
    "            'max_iter': [5000, 7000, 10000]\n",
    "        },\n",
    "        # For 'saga' solver: supports 'l1', 'l2', and 'elasticnet' penalties\n",
    "        {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'l1_ratio': [0.5],  # Only relevant for 'elasticnet'\n",
    "            'max_iter': [5000, 7000, 10000]\n",
    "        }\n",
    "    ],\n",
    "    'mlp': {\n",
    "        'hidden_layer_sizes': [(64,), (128,), (128, 64), (128, 64, 32)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['adam', 'sgd'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'max_iter': [300, 500, 1000]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 6, 9],\n",
    "        'subsample': [0.7, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "        'gamma': [0, 0.1, 0.3],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Trova il miglior classificatore unendo train e validation\n",
    "    best_clf, best_params = find_best_classifier(X_train, y_train, X_val, y_val, classifier_name, param_grid[classifier_name])\n",
    "    \n",
    "    # Test sul test set\n",
    "    print(\"Test metrics:\")\n",
    "    test_with_voting(best_clf, X_test, y_test, voting_type)\n",
    "    \n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m classificatori \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmlp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m voting \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajority\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_patients_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_patients_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassificatori\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvoting\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, classifier_name, voting_type)\u001b[0m\n\u001b[0;32m      3\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_forest\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m200\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m }\n\u001b[0;32m     50\u001b[0m }\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Trova il miglior classificatore unendo train e validation\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m best_clf, best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclassifier_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Test sul test set\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m, in \u001b[0;36mfind_best_classifier\u001b[1;34m(X_train, y_train, X_val, y_val, classifier_name, param_grid)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ricerca dei migliori parametri con GridSearchCV\u001b[39;00m\n\u001b[0;32m     13\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(classifier, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Ritorna il miglior classificatore\u001b[39;00m\n\u001b[0;32m     17\u001b[0m best_clf \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1572\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bsbar\\anaconda3\\envs\\iml\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test\n",
    "classificatori = ['random_forest', 'mlp', 'svm', 'xgboost', 'logistic']\n",
    "voting = ['majority', 'mean']\n",
    "best_params = run_experiment(x_train_expanded, y_train_expanded, final_patients_val, y_val, final_patients_test, y_test, classificatori[2], voting[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
