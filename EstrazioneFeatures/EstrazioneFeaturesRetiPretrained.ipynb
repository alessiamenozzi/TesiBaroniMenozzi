{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estrazione features da reti pretrainate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import nrrd\n",
    "import os\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import nrrd\n",
    "import collections\n",
    "from keras.preprocessing import image\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D VGG CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "# Patientes folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "# Function to find coordinates\n",
    "def maskcroppingbox(mask_slice):\n",
    "    non_zero_coords = np.argwhere(mask_slice)\n",
    "    (ystart, xstart), (ystop, xstop) = non_zero_coords.min(axis=0), non_zero_coords.max(axis=0) + 1\n",
    "    return (ystart, xstart), (ystop, xstop)\n",
    "\n",
    "# Preprocess of slices\n",
    "def extract_squares_for_all_slices(image_array, mask_array, target_size=224):\n",
    "    slices_with_plaque = []\n",
    "\n",
    "    # Iterate over all slices\n",
    "    for i in range(image_array.shape[0]):\n",
    "        mask_slice = mask_array[i]\n",
    "        image_slice = image_array[i]\n",
    "\n",
    "        if np.sum(mask_slice) > 0:\n",
    "            (ystart, xstart), (ystop, xstop) = maskcroppingbox(mask_slice)\n",
    "\n",
    "            # Find the center of the plaque\n",
    "            center_y = (ystart + ystop) // 2\n",
    "            center_x = (xstart + xstop) // 2\n",
    "\n",
    "            # Crop the image around the plaque\n",
    "            half_side = min(center_y, center_x, 30) \n",
    "            roi_image = image_slice[\n",
    "                max(0, center_y-half_side):min(center_y+half_side, image_slice.shape[0]), \n",
    "                max(0, center_x-half_side):min(center_x+half_side, image_slice.shape[1])\n",
    "            ]\n",
    "            \n",
    "            # Resize to 224*224\n",
    "            roi_image_resized = resize(roi_image, (target_size, target_size), order=1, anti_aliasing=True)\n",
    "\n",
    "        else:\n",
    "            roi_image_resized = image_array[i]\n",
    "\n",
    "        slices_with_plaque.append(roi_image_resized)\n",
    "\n",
    "    return slices_with_plaque\n",
    "\n",
    "# Extract features\n",
    "def featureextraction(image_array, mask_array):\n",
    "    roi_images_resized = extract_squares_for_all_slices(image_array, mask_array)\n",
    "\n",
    "    # Find the largest slice\n",
    "    slice_sums = np.sum(mask_array, axis=(1, 2))\n",
    "    largest_slice_index = np.argmax(slice_sums)\n",
    "    largest_slice_image = roi_images_resized[largest_slice_index]\n",
    "    #print(image_array.shape)\n",
    "    x = image.img_to_array(largest_slice_image)\n",
    "\n",
    "    # Repeat the channel\n",
    "    x = np.repeat(x, 3, axis=-1)  \n",
    "    # Add batch dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    # preprocess using the model's function\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # extract the feature maps\n",
    "    base_model_pool_features = model.predict(x)\n",
    "    feature_map = base_model_pool_features[0]\n",
    "\n",
    "    feature_map = feature_map.transpose((2, 1, 0))\n",
    "\n",
    "    ## Global Max Pooling\n",
    "    features = np.max(feature_map, -1)\n",
    "    features = np.max(features, -1)\n",
    "    # Collect the features\n",
    "    deeplearningfeatures = collections.OrderedDict()\n",
    "    for ind_, f_ in enumerate(features):\n",
    "        deeplearningfeatures[str(ind_)] = f_\n",
    "\n",
    "    return deeplearningfeatures\n",
    "\n",
    "\n",
    "# Main cycle\n",
    "featureDict = {}\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(s)\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "        \n",
    "    for t in os.listdir(filename):\n",
    "\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    \n",
    "    deeplearningfeatures = featureextraction(readdatadicom,readdatanrrd) \n",
    "\n",
    "    result = deeplearningfeatures\n",
    "    key = list(result.keys())\n",
    "    key = key[0:]\n",
    "        \n",
    "    feature = []\n",
    "    for jind in range(len(key)):\n",
    "        feature.append(result[key[jind]])\n",
    "        \n",
    "    featureDict[s] = feature\n",
    "    dictkey = key\n",
    "    print(s)\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame.from_dict(featureDict, orient='index', columns=dictkey)\n",
    "dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCEPTION_SingolaSlice_Imagenet_Ritagliata.csv')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D VGG FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=False)\n",
    "# Patients folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "# Extract features\n",
    "def featureextraction(image_array, mask_array):\n",
    "\n",
    "    roi_images = image_array \n",
    "    mask_slices = mask_array\n",
    "    \n",
    "    # Lagest slice\n",
    "    slice_sums = np.sum(mask_slices, axis=(1, 2)) \n",
    "    largest_slice_index = np.argmax(slice_sums)\n",
    "\n",
    "    print(largest_slice_index)\n",
    "    print(roi_images.shape)\n",
    "\n",
    "    largest_slice_image = roi_images[largest_slice_index]\n",
    "\n",
    "    x = image.img_to_array(largest_slice_image)\n",
    "    # Resize to the correct dimension\n",
    "    x = cv2.resize(x, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "    x = np.repeat(x[..., np.newaxis], 3, axis=-1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    # Extract the fetaure maps\n",
    "    base_model_pool_features = model.predict(x)\n",
    "\n",
    "    feature_map = base_model_pool_features[0]\n",
    "    feature_map = feature_map.transpose((2, 1, 0))\n",
    "    \n",
    "    ## Global Max Pooling\n",
    "    features = np.max(feature_map, -1)\n",
    "    features = np.max(features, -1)\n",
    "\n",
    "    deeplearningfeatures = collections.OrderedDict()\n",
    "    \n",
    "    for ind_, f_ in enumerate(features):\n",
    "        deeplearningfeatures[str(ind_)] = f_\n",
    "         \n",
    "    return deeplearningfeatures\n",
    "\n",
    "\n",
    "# Main cycle\n",
    "featureDict = {}\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(s)\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "        \n",
    "    for t in os.listdir(filename):\n",
    "\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    \n",
    "    deeplearningfeatures = featureextraction(readdatadicom,readdatanrrd) \n",
    "\n",
    "    result = deeplearningfeatures\n",
    "    key = list(result.keys())\n",
    "    key = key[0:]\n",
    "        \n",
    "    feature = []\n",
    "    for jind in range(len(key)):\n",
    "        feature.append(result[key[jind]])\n",
    "        \n",
    "    featureDict[s] = feature\n",
    "    dictkey = key\n",
    "    print(s)\n",
    "    \n",
    "dataframe = pd.DataFrame.from_dict(featureDict, orient='index', columns=dictkey)\n",
    "dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCEPTION_SingolaSlice_Imagenet_fullimage.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RADIMAGENET 2D FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../PreTrained_Models/RadImageNet-IRV2_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-ResNet50_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "\n",
    "# Load the pre trained model, you can choose the network by copy and pasting the paths above\n",
    "model_path = \"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Patients folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "# Extract features\n",
    "def featureextraction(image_array, mask_array):\n",
    "\n",
    "    # Largest slice\n",
    "    slice_sums = np.sum(mask_array, axis=(1, 2))\n",
    "    largest_slice_index = np.argmax(slice_sums)\n",
    "    largest_slice_image = image_array[largest_slice_index]\n",
    "    x = image.img_to_array(largest_slice_image)\n",
    "    min_val = np.min(x)\n",
    "    max_val = np.max(x)\n",
    "\n",
    "    # Min max normalization\n",
    "    x = (x - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Resize to 224*224\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "\n",
    "    # Add channel dimension\n",
    "    x = np.expand_dims(x, axis=-1)\n",
    "\n",
    "    # Repeat the channel\n",
    "    x = np.repeat(x, 3, axis=-1)    \n",
    "    \n",
    "    # Add batch dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    # Extract the features\n",
    "    base_model_pool_features = model.predict(x)\n",
    "    feature_map = base_model_pool_features[0]\n",
    "    feature_map = feature_map.transpose((2, 1, 0))\n",
    "    \n",
    "    ## Global Max Pooling\n",
    "    features = np.max(feature_map, -1)\n",
    "    features = np.max(features, -1)\n",
    "    \n",
    "    deeplearningfeatures = collections.OrderedDict()\n",
    "    for ind_, f_ in enumerate(features):\n",
    "        deeplearningfeatures[str(ind_)] = f_\n",
    "\n",
    "    return deeplearningfeatures\n",
    "\n",
    "# Main cycle\n",
    "featureDict = {}\n",
    "for s in os.listdir(pathdicom):\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    deeplearningfeatures = featureextraction(readdatadicom, readdatanrrd)\n",
    "\n",
    "    result = deeplearningfeatures\n",
    "    key = list(result.keys())\n",
    "\n",
    "    feature = []\n",
    "    for jind in range(len(key)):\n",
    "        feature.append(result[key[jind]])\n",
    "\n",
    "    featureDict[s] = feature\n",
    "    dictkey = key\n",
    "    print(s)\n",
    "\n",
    "#dataframe = pd.DataFrame.from_dict(featureDict, orient='index', columns=dictkey)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCEPTION_SliceMaggiore_NuoviPesi_fullimage_GAP.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RADIMAGENET 2D CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../PreTrained_Models/RadImageNet-IRV2_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-ResNet50_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "\n",
    "# Load the pre trained model, you can choose the network by copy and pasting the paths above\n",
    "model_path = \"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Patients folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "# Find coordinates\n",
    "def maskcroppingbox(mask_slice):\n",
    "    non_zero_coords = np.argwhere(mask_slice)\n",
    "    (ystart, xstart), (ystop, xstop) = non_zero_coords.min(axis=0), non_zero_coords.max(axis=0) + 1\n",
    "    return (ystart, xstart), (ystop, xstop)\n",
    "\n",
    "# Preprocess the slices\n",
    "def extract_squares_for_all_slices(image_array, mask_array, target_size=224):\n",
    "    slices_with_plaque = []\n",
    "    for i in range(image_array.shape[0]):\n",
    "        mask_slice = mask_array[i]\n",
    "        image_slice = image_array[i]\n",
    "\n",
    "        if np.sum(mask_slice) > 0:\n",
    "            (ystart, xstart), (ystop, xstop) = maskcroppingbox(mask_slice)\n",
    "            center_y = (ystart + ystop) // 2\n",
    "            center_x = (xstart + xstop) // 2\n",
    "\n",
    "            half_side = min(center_y, center_x, 30)\n",
    "\n",
    "            # min max normalization\n",
    "            image_slice_normalized = (image_slice - np.min(image_slice)) / (np.max(image_slice) - np.min(image_slice)) \n",
    "            roi_image = image_slice_normalized[\n",
    "                max(0, center_y-half_side):min(center_y+half_side, image_slice.shape[0]), \n",
    "                max(0, center_x-half_side):min(center_x+half_side, image_slice.shape[1])\n",
    "            ]\n",
    "            \n",
    "            # Resize\n",
    "            roi_image_resized = resize(roi_image, (target_size, target_size), order=1, anti_aliasing=True)\n",
    "\n",
    "        else:\n",
    "            roi_image_resized = image_array[i]\n",
    "\n",
    "        slices_with_plaque.append(roi_image_resized)\n",
    "\n",
    "    return slices_with_plaque\n",
    "\n",
    "# Extract features\n",
    "def featureextraction(image_array, mask_array):\n",
    "    roi_images_resized = extract_squares_for_all_slices(image_array, mask_array)\n",
    "\n",
    "    # Find the largest slice\n",
    "    slice_sums = np.sum(mask_array, axis=(1, 2))\n",
    "    largest_slice_index = np.argmax(slice_sums)\n",
    "    largest_slice_image = roi_images_resized[largest_slice_index]\n",
    "    x = image.img_to_array(largest_slice_image)\n",
    "\n",
    "    # Repeat the channel\n",
    "    x = np.repeat(x, 3, axis=-1)    \n",
    "    \n",
    "    # Add batch dimension\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Estrazione delle feature principali\n",
    "    base_model_pool_features = model.predict(x)\n",
    "    feature_map = base_model_pool_features[0]\n",
    "\n",
    "    feature_map = feature_map.transpose((2, 1, 0))\n",
    "\n",
    "    ## Global Max Pooling\n",
    "    features = np.max(feature_map, -1)\n",
    "    features = np.max(features, -1)\n",
    "\n",
    "    deeplearningfeatures = collections.OrderedDict()\n",
    "    for ind_, f_ in enumerate(features):\n",
    "        deeplearningfeatures[str(ind_)] = f_\n",
    "\n",
    "    return deeplearningfeatures\n",
    "\n",
    "\n",
    "# Main cycle\n",
    "featureDict = {}\n",
    "for s in os.listdir(pathdicom):\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    deeplearningfeatures = featureextraction(readdatadicom, readdatanrrd)\n",
    "\n",
    "    result = deeplearningfeatures\n",
    "    key = list(result.keys())\n",
    "\n",
    "    feature = []\n",
    "    for jind in range(len(key)):\n",
    "        feature.append(result[key[jind]])\n",
    "\n",
    "    featureDict[s] = feature\n",
    "    dictkey = key\n",
    "    print(s)\n",
    "\n",
    "#dataframe = pd.DataFrame.from_dict(featureDict, orient='index', columns=dictkey)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\RESNET_SliceMaggiore_NuoviPesi_Ritagliata.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE SLICE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5D VGG CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "\n",
    "def maskcroppingbox(mask_slice):\n",
    "    non_zero_coords = np.argwhere(mask_slice)\n",
    "    (ystart, xstart), (ystop, xstop) = non_zero_coords.min(axis=0), non_zero_coords.max(axis=0) + 1\n",
    "    return (ystart, xstart), (ystop, xstop)\n",
    "\n",
    "def featureextraction(image_array, mask_array, patient_id):\n",
    "    # List to contain all the slices\n",
    "    batch_slices = []\n",
    "    original_slice_indices = []\n",
    "\n",
    "    # Iterate over every slice\n",
    "    for slice_idx in range(image_array.shape[0]): \n",
    "        image_slice = image_array[slice_idx]\n",
    "        mask_slice = mask_array[slice_idx]\n",
    "\n",
    "        # If the mask is not empty preprocess the slice\n",
    "        if np.any(mask_slice):\n",
    "            \n",
    "            (ystart, xstart), (ystop, xstop) = maskcroppingbox(mask_slice)\n",
    "            center_y = (ystart + ystop) // 2\n",
    "            center_x = (xstart + xstop) // 2\n",
    "            half_side = min(center_y, center_x, 30)\n",
    "            roi_image = image_slice[\n",
    "                max(0, center_y-half_side):min(center_y+half_side, image_slice.shape[0]), \n",
    "                max(0, center_x-half_side):min(center_x+half_side, image_slice.shape[1])\n",
    "            ]\n",
    "            roi_image = roi_image.astype(np.float32)\n",
    "            roi_image_resized = resize(roi_image, (224, 224), order=1, anti_aliasing=True)\n",
    "\n",
    "            x = image.img_to_array(roi_image_resized)\n",
    "            x = np.repeat(x, 3, axis=-1) \n",
    "\n",
    "            x = preprocess_input(x)\n",
    "            batch_slices.append(x)\n",
    "            # Save the index for the csv\n",
    "            original_slice_indices.append(slice_idx)\n",
    "\n",
    "    batch_slices = np.array(batch_slices)\n",
    "\n",
    "    # The whole batch is passed to the pre trained network\n",
    "    base_model_pool_features = model.predict(batch_slices)\n",
    "\n",
    "    all_features = []\n",
    "\n",
    "    # Consider only the slices based on the original index\n",
    "    for i in range(base_model_pool_features.shape[0]):\n",
    "        feature_map = base_model_pool_features[i]\n",
    "        feature_map = feature_map.transpose((2, 1, 0))\n",
    "        \n",
    "        ## Global Max Pooling\n",
    "        features = np.max(feature_map, -1)\n",
    "        features = np.max(features, -1)\n",
    "\n",
    "        feature_entry = {'Patient': patient_id, 'Slice': original_slice_indices[i]}\n",
    "        for ind_, f_ in enumerate(features):\n",
    "            feature_entry[f'Feature_{ind_}'] = f_\n",
    "\n",
    "        all_features.append(feature_entry)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "all_feature_dicts = []\n",
    "\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(f\"Processing patient: {s}\")\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "    patient_features = featureextraction(readdatadicom, readdatanrrd, patient_id=s)\n",
    "    all_feature_dicts.extend(patient_features)\n",
    "\n",
    "# Create and save the final csv\n",
    "#dataframe = pd.DataFrame(all_feature_dicts)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\RESNET_ALL_SLICES_Imagenet_ritagliata.csv', index=False)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5D VGG FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=False)\n",
    "\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "def featureextraction(image_array, mask_array, patient_id):\n",
    "    batch_slices = []\n",
    "    original_slice_indices = []\n",
    "\n",
    "    for slice_idx in range(image_array.shape[0]):\n",
    "        image_slice = image_array[slice_idx]\n",
    "        mask_slice = mask_array[slice_idx]\n",
    "\n",
    "        if np.any(mask_slice):\n",
    "            \n",
    "            # Resize and preprocessing\n",
    "            image_slice = image_slice.astype(np.float32)\n",
    "            roi_image_resized = cv2.resize(image_slice, (224, 224), interpolation=cv2.INTER_LINEAR)\n",
    "            x = image.img_to_array(roi_image_resized)\n",
    "            x = np.repeat(x, 3, axis=-1) \n",
    "            x = preprocess_input(x)\n",
    "            batch_slices.append(x)\n",
    "            original_slice_indices.append(slice_idx)\n",
    "\n",
    "    batch_slices = np.array(batch_slices)\n",
    "    # Extract the feature maps\n",
    "    base_model_pool_features = model.predict(batch_slices)\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(base_model_pool_features.shape[0]):\n",
    "        feature_map = base_model_pool_features[i]\n",
    "        feature_map = feature_map.transpose((2, 1, 0))\n",
    "        \n",
    "        ## Global Max Pooling\n",
    "        features = np.max(feature_map, -1)\n",
    "        features = np.max(features, -1)\n",
    "\n",
    "        feature_entry = {'Patient': patient_id, 'Slice': original_slice_indices[i]}\n",
    "        for ind_, f_ in enumerate(features):\n",
    "            feature_entry[f'Feature_{ind_}'] = f_\n",
    "\n",
    "        all_features.append(feature_entry)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "all_feature_dicts = []\n",
    "\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(f\"Processing patient: {s}\")\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    patient_features = featureextraction(readdatadicom, readdatanrrd, patient_id=s)\n",
    "    all_feature_dicts.extend(patient_features)\n",
    "\n",
    "# create and save the final csv\n",
    "#dataframe = pd.DataFrame(all_feature_dicts)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCEPTION_ALL_SLICES_Imagenet_fullimage.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RADIMAGENET 2.5D CUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../PreTrained_Models/RadImageNet-IRV2_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-ResNet50_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "\n",
    "# Load the pre trained model, you can choose the network by copy and pasting the paths above\n",
    "model_path = \"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Patients folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "def maskcroppingbox(mask_slice):\n",
    "    non_zero_coords = np.argwhere(mask_slice)\n",
    "    (ystart, xstart), (ystop, xstop) = non_zero_coords.min(axis=0), non_zero_coords.max(axis=0) + 1\n",
    "    return (ystart, xstart), (ystop, xstop)\n",
    "\n",
    "def featureextraction(image_array, mask_array, patient_id):\n",
    "    batch_slices = []\n",
    "    original_slice_indices = []\n",
    "\n",
    "    for slice_idx in range(image_array.shape[0]):\n",
    "        image_slice = image_array[slice_idx]\n",
    "        mask_slice = mask_array[slice_idx]\n",
    "\n",
    "        if np.any(mask_slice):\n",
    "            (ystart, xstart), (ystop, xstop) = maskcroppingbox(mask_slice)\n",
    "            center_y = (ystart + ystop) // 2\n",
    "            center_x = (xstart + xstop) // 2\n",
    "\n",
    "            half_side = min(center_y, center_x, 30)\n",
    "\n",
    "            # min max norm and create the final slice\n",
    "            image_slice_normalized = (image_slice - np.min(image_slice)) / (np.max(image_slice) - np.min(image_slice)) \n",
    "            roi_image = image_slice_normalized[\n",
    "                max(0, center_y-half_side):min(center_y+half_side, image_slice.shape[0]), \n",
    "                max(0, center_x-half_side):min(center_x+half_side, image_slice.shape[1])\n",
    "            ]\n",
    "            \n",
    "            roi_image_resized = resize(roi_image, (224, 224), order=1, anti_aliasing=True)\n",
    "            x = image.img_to_array(roi_image_resized)\n",
    "            x = np.repeat(x, 3, axis=-1)          \n",
    "            batch_slices.append(x)\n",
    "            original_slice_indices.append(slice_idx)\n",
    "\n",
    "    batch_slices = np.array(batch_slices)\n",
    "\n",
    "    base_model_pool_features = model.predict(batch_slices)\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(base_model_pool_features.shape[0]):\n",
    "        feature_map = base_model_pool_features[i]\n",
    "        feature_map = feature_map.transpose((2, 1, 0))\n",
    "        \n",
    "        ## Global Max Pooling\n",
    "        features = np.max(feature_map, -1)\n",
    "        features = np.max(features, -1)\n",
    "\n",
    "        feature_entry = {'Patient': patient_id, 'Slice': original_slice_indices[i]}\n",
    "        for ind_, f_ in enumerate(features):\n",
    "            feature_entry[f'Feature_{ind_}'] = f_\n",
    "\n",
    "        all_features.append(feature_entry)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "all_feature_dicts = []\n",
    "\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(f\"Processing patient: {s}\")\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    patient_features = featureextraction(readdatadicom, readdatanrrd, patient_id=s)\n",
    "    all_feature_dicts.extend(patient_features)\n",
    "\n",
    "# Create and save\n",
    "#dataframe = pd.DataFrame(all_feature_dicts)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCRES_ALL_SLICES_NuoviPesi_Ritagliata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RADIMAGENET 2.5D FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"../PreTrained_Models/RadImageNet-IRV2_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-ResNet50_notop.h5\"\n",
    "#\"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "\n",
    "# Load the pre trained model, you can choose the network by copy and pasting the paths above\n",
    "model_path = \"../PreTrained_Models/RadImageNet-InceptionV3_notop.h5\"\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Patients folder\n",
    "pathdicom = \"\\\\Users\\\\bsbar\\\\Desktop\\\\pazienti_nrrd\"\n",
    "pathroi = \"\\\\Users\\\\bsbar\\\\Desktop\\\\Tesi\\\\ROI\"\n",
    "\n",
    "def featureextraction(image_array, mask_array, patient_id):\n",
    "    batch_slices = []\n",
    "    original_slice_indices = []\n",
    "\n",
    "    for slice_idx in range(image_array.shape[0]):\n",
    "        image_slice = image_array[slice_idx]\n",
    "        mask_slice = mask_array[slice_idx]\n",
    "\n",
    "        if np.any(mask_slice):\n",
    "            image_slice = image_slice.astype(np.float32)\n",
    "            roi_image_resized = x = cv2.resize(image_slice, (224, 224))\n",
    "            x = image.img_to_array(roi_image_resized)\n",
    "            min_val = np.min(x)\n",
    "            max_val = np.max(x)\n",
    "\n",
    "            # min max norm\n",
    "            x = (x - min_val) / (max_val - min_val)\n",
    "            x = np.repeat(x, 3, axis=-1) \n",
    "\n",
    "            batch_slices.append(x) \n",
    "            original_slice_indices.append(slice_idx)\n",
    "\n",
    "    batch_slices = np.array(batch_slices)\n",
    "\n",
    "    base_model_pool_features = model.predict(batch_slices)\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(base_model_pool_features.shape[0]):\n",
    "        feature_map = base_model_pool_features[i]\n",
    "        feature_map = feature_map.transpose((2, 1, 0))\n",
    "        \n",
    "        ## Global Max Pooling\n",
    "        features = np.max(feature_map, -1)\n",
    "        features = np.max(features, -1)\n",
    "\n",
    "        feature_entry = {'Patient': patient_id, 'Slice': original_slice_indices[i]}\n",
    "        for ind_, f_ in enumerate(features):\n",
    "            feature_entry[f'Feature_{ind_}'] = f_\n",
    "\n",
    "        all_features.append(feature_entry)\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "all_feature_dicts = []\n",
    "\n",
    "for s in os.listdir(pathdicom):\n",
    "    print(f\"Processing patient: {s}\")\n",
    "    filename = os.path.join(pathdicom, s)\n",
    "\n",
    "    for t in os.listdir(filename):\n",
    "        pathdicomnew = os.path.join(pathdicom, s, t)\n",
    "        readdatadicom, header = nrrd.read(pathdicomnew, index_order='C')\n",
    "\n",
    "    pathroinew = os.path.join(pathroi, s)\n",
    "    for g in os.listdir(pathroinew):\n",
    "        troi = os.path.join(pathroi, s, g)\n",
    "        readdatanrrd, header2 = nrrd.read(troi, index_order='C')\n",
    "\n",
    "    patient_features = featureextraction(readdatadicom, readdatanrrd, patient_id=s)\n",
    "    all_feature_dicts.extend(patient_features)\n",
    "\n",
    "# Create and save csv\n",
    "#dataframe = pd.DataFrame(all_feature_dicts)\n",
    "#dataframe.to_csv('C:\\\\Users\\\\bsbar\\\\Desktop\\\\INCEPTION_ALL_SLICES_NuoviPesi_FullImage.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
